#!/usr/bin/env python3
"""
Script d'√©valuation compl√®te du mod√®le final ADAN.

Ce script effectue :
- Validation du mod√®le entra√Æn√©
- Backtesting hors √©chantillon sur 3 mois
- Calcul des m√©triques de performance
- G√©n√©ration d'un rapport complet
- Tests sur diff√©rents r√©gimes de march√©
"""

import argparse
import os
import sys
import logging
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings

warnings.filterwarnings("ignore")

# Ajouter le path du projet
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

import torch
from stable_baselines3 import PPO

from adan_trading_bot.common.config_loader import ConfigLoader
from adan_trading_bot.common.custom_logger import setup_logging
from adan_trading_bot.data_processing.data_loader import ChunkedDataLoader
from adan_trading_bot.environment.multi_asset_chunked_env import MultiAssetChunkedEnv
from adan_trading_bot.model.model_ensemble import ModelEnsemble


class ModelEvaluator:
    """Classe principale pour l'√©valuation compl√®te du mod√®le final."""

    def __init__(self, config_path: str, model_path: str):
        """
        Initialise l'√©valuateur de mod√®le.

        Args:
            config_path: Chemin vers le fichier de configuration
            model_path: Chemin vers le mod√®le entra√Æn√©
        """
        self.logger = logging.getLogger(__name__)
        self.config = ConfigLoader.load_config(config_path)
        self.model_path = model_path
        self.model = None
        self.env = None
        self.results = {}

        # M√©triques de performance
        self.performance_metrics = {
            "total_return": 0.0,
            "sharpe_ratio": 0.0,
            "max_drawdown": 0.0,
            "win_rate": 0.0,
            "profit_factor": 0.0,
            "total_trades": 0,
            "winning_trades": 0,
            "losing_trades": 0,
            "average_trade": 0.0,
            "best_trade": 0.0,
            "worst_trade": 0.0,
            "volatility": 0.0,
            "calmar_ratio": 0.0,
            "sortino_ratio": 0.0,
        }

    def load_model(self) -> bool:
        """
        Charge le mod√®le entra√Æn√©.

        Returns:
            bool: True si chargement r√©ussi, False sinon
        """
        try:
            if not os.path.exists(self.model_path):
                self.logger.error(f"‚ùå Mod√®le non trouv√©: {self.model_path}")
                return False

            self.logger.info(f"üîÑ Chargement du mod√®le: {self.model_path}")

            # V√©rifier l'extension du fichier
            if self.model_path.endswith(".zip"):
                self.model = PPO.load(self.model_path)
                self.logger.info("‚úÖ Mod√®le PPO charg√© avec succ√®s")
            elif self.model_path.endswith(".pth"):
                # Charger le fichier torch
                checkpoint = torch.load(self.model_path, map_location="cpu")
                self.logger.info("‚úÖ Checkpoint PyTorch charg√©")

                # Afficher les informations du mod√®le
                if "fusion_weights" in checkpoint:
                    fusion_weights = checkpoint["fusion_weights"]
                    self.logger.info(f"üéØ Poids de fusion: {fusion_weights}")

                if "hyperparameters" in checkpoint:
                    hyperparams = checkpoint["hyperparameters"]
                    self.logger.info(f"‚öôÔ∏è Hyperparam√®tres: {hyperparams}")

            return True

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            return False

    def setup_environment(self, test_period_months: int = 3) -> bool:
        """
        Configure l'environnement pour le backtesting.

        Args:
            test_period_months: P√©riode de test en mois

        Returns:
            bool: True si configuration r√©ussie, False sinon
        """
        try:
            self.logger.info(
                f"üîÑ Configuration environnement de test ({test_period_months} mois)..."
            )

            # Configurer les dates pour le backtesting hors √©chantillon
            end_date = datetime.now()
            start_date = end_date - timedelta(days=test_period_months * 30)

            self.logger.info(
                f"üìÖ P√©riode de test: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}"
            )

            # Cr√©er le data loader pour les donn√©es de test
            test_config = self.config["workers"]["w1"].copy()
            test_config["chunk_size"] = 10000  # Taille r√©duite pour validation

            data_loader = ChunkedDataLoader(
                config=self.config, worker_config=test_config, worker_id=0
            )

            # Charger les donn√©es de test
            test_data = data_loader.load_chunk(0)  # Premier chunk pour test

            if not test_data or not any(test_data.values()):
                self.logger.error("‚ùå Aucune donn√©e de test disponible")
                return False

            # Calculer le nombre total d'√©chantillons pour le log
            total_samples = sum(len(df) for asset_data in test_data.values() for df in asset_data.values())

            self.logger.info(
                f"üìä Donn√©es de test charg√©es: {total_samples} √©chantillons pour {len(test_data)} actifs"
            )

            # Cr√©er l'environnement de backtesting
            self.env = MultiAssetChunkedEnv(
                data=test_data, config=self.config, worker_config=test_config, timeframes=self.config["data"]["timeframes"], window_size=self.config["environment"]["window_size"], features_config=self.config["data"]["features_config"]["timeframes"]
            )

            self.logger.info("‚úÖ Environnement de test configur√©")
            return True

        except Exception as e:
            self.logger.error(f"‚ùå Erreur configuration environnement: {e}")
            return False

    def run_backtest(self, num_episodes: int = 10) -> Dict:
        """
        Ex√©cute le backtesting du mod√®le.

        Args:
            num_episodes: Nombre d'√©pisodes de test

        Returns:
            dict: R√©sultats du backtesting
        """
        if not self.model or not self.env:
            self.logger.error("‚ùå Mod√®le ou environnement non initialis√©")
            return {}

        try:
            self.logger.info(f"üöÄ D√©marrage backtesting ({num_episodes} √©pisodes)...")

            all_rewards = []
            all_returns = []
            all_actions = []
            portfolio_values = []

            for episode in range(num_episodes):
                self.logger.info(f"üìà Episode {episode + 1}/{num_episodes}")

                obs, _ = self.env.reset()
                episode_rewards = []
                episode_returns = []
                episode_actions = []
                done = False
                step = 0

                while not done and step < 1000:  # Limite de s√©curit√©
                    if isinstance(self.model, PPO):
                        action, _ = self.model.predict(obs, deterministic=True)
                    else:
                        # Pour les mod√®les custom, adapter selon l'interface
                        action = self.env.action_space.sample()

                    obs, reward, terminated, truncated, info = self.env.step(action)
                    done = terminated or truncated

                    episode_rewards.append(reward)
                    episode_actions.append(action)

                    # R√©cup√©rer les informations de l'environnement
                    if "portfolio_value" in info:
                        portfolio_values.append(info["portfolio_value"])
                    if "return" in info:
                        episode_returns.append(info["return"])

                    step += 1

                all_rewards.extend(episode_rewards)
                all_returns.extend(episode_returns)
                all_actions.extend(episode_actions)

                self.logger.info(
                    f"‚úÖ Episode {episode + 1} termin√© - {step} steps, reward total: {sum(episode_rewards):.4f}"
                )

            # Calculer les m√©triques de performance
            self.calculate_performance_metrics(
                all_rewards, all_returns, portfolio_values
            )

            backtest_results = {
                "total_episodes": num_episodes,
                "total_steps": len(all_rewards),
                "average_reward": np.mean(all_rewards) if all_rewards else 0.0,
                "total_reward": sum(all_rewards) if all_rewards else 0.0,
                "reward_std": np.std(all_rewards) if all_rewards else 0.0,
                "returns": all_returns,
                "actions": all_actions,
                "portfolio_values": portfolio_values,
                "performance_metrics": self.performance_metrics,
            }

            self.logger.info("‚úÖ Backtesting termin√© avec succ√®s")
            return backtest_results

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors du backtesting: {e}")
            return {}

    def calculate_performance_metrics(
        self, rewards: List[float], returns: List[float], portfolio_values: List[float]
    ):
        """
        Calcule les m√©triques de performance d√©taill√©es.

        Args:
            rewards: Liste des rewards
            returns: Liste des returns
            portfolio_values: Liste des valeurs de portfolio
        """
        try:
            if not returns or len(returns) == 0:
                self.logger.warning(
                    "‚ö†Ô∏è Pas de donn√©es de returns pour calculer les m√©triques"
                )
                return

            returns_array = np.array(returns)

            # Return total
            total_return = (
                (returns_array + 1).prod() - 1 if len(returns_array) > 0 else 0.0
            )
            self.performance_metrics["total_return"] = total_return

            # Volatilit√©
            volatility = (
                np.std(returns_array) * np.sqrt(252) if len(returns_array) > 1 else 0.0
            )
            self.performance_metrics["volatility"] = volatility

            # Sharpe ratio
            mean_return = np.mean(returns_array)
            if volatility > 0:
                sharpe_ratio = (mean_return * 252) / volatility
                self.performance_metrics["sharpe_ratio"] = sharpe_ratio

            # Maximum drawdown
            if portfolio_values:
                portfolio_array = np.array(portfolio_values)
                cumulative_max = np.maximum.accumulate(portfolio_array)
                drawdowns = (portfolio_array - cumulative_max) / cumulative_max
                max_drawdown = np.min(drawdowns) if len(drawdowns) > 0 else 0.0
                self.performance_metrics["max_drawdown"] = abs(max_drawdown)

                # Calmar ratio
                if max_drawdown < 0:
                    calmar_ratio = (mean_return * 252) / abs(max_drawdown)
                    self.performance_metrics["calmar_ratio"] = calmar_ratio

            # Sortino ratio (downside deviation)
            downside_returns = returns_array[returns_array < 0]
            if len(downside_returns) > 0:
                downside_dev = np.std(downside_returns) * np.sqrt(252)
                if downside_dev > 0:
                    sortino_ratio = (mean_return * 252) / downside_dev
                    self.performance_metrics["sortino_ratio"] = sortino_ratio

            # Win rate et statistiques des trades
            winning_trades = len(returns_array[returns_array > 0])
            losing_trades = len(returns_array[returns_array < 0])
            total_trades = len(returns_array)

            self.performance_metrics.update(
                {
                    "total_trades": total_trades,
                    "winning_trades": winning_trades,
                    "losing_trades": losing_trades,
                    "win_rate": (winning_trades / total_trades * 100)
                    if total_trades > 0
                    else 0.0,
                    "average_trade": mean_return,
                    "best_trade": np.max(returns_array)
                    if len(returns_array) > 0
                    else 0.0,
                    "worst_trade": np.min(returns_array)
                    if len(returns_array) > 0
                    else 0.0,
                }
            )

            # Profit factor
            gross_profit = np.sum(returns_array[returns_array > 0])
            gross_loss = abs(np.sum(returns_array[returns_array < 0]))
            if gross_loss > 0:
                profit_factor = gross_profit / gross_loss
                self.performance_metrics["profit_factor"] = profit_factor

        except Exception as e:
            self.logger.error(f"‚ùå Erreur calcul m√©triques: {e}")

    def validate_fusion_weights(self) -> bool:
        """
        Valide les poids de fusion optimaux.

        Returns:
            bool: True si poids valides, False sinon
        """
        try:
            self.logger.info("üîç Validation des poids de fusion...")

            # Poids attendus selon l'optimisation Optuna
            expected_weights = {0: 0.25, 1: 0.27, 2: 0.30, 3: 0.18}

            # Cr√©er une instance ModelEnsemble pour r√©cup√©rer les poids
            ensemble = ModelEnsemble()
            actual_weights = ensemble.get_fusion_weights()

            valid_weights = 0
            tolerance = 0.01

            for worker_idx, expected_weight in expected_weights.items():
                actual_weight = actual_weights.get(worker_idx, 0.0)
                if abs(actual_weight - expected_weight) < tolerance:
                    valid_weights += 1
                    self.logger.info(
                        f"‚úÖ Worker {worker_idx}: {actual_weight:.3f} (attendu: {expected_weight:.3f})"
                    )
                else:
                    self.logger.warning(
                        f"‚ö†Ô∏è Worker {worker_idx}: {actual_weight:.3f} vs attendu: {expected_weight:.3f}"
                    )

            success = valid_weights == 4
            self.logger.info(f"üéØ Validation poids: {valid_weights}/4 workers corrects")

            return success

        except Exception as e:
            self.logger.error(f"‚ùå Erreur validation poids fusion: {e}")
            return False

    def generate_report(self, output_path: str = None) -> str:
        """
        G√©n√®re un rapport complet d'√©valuation.

        Args:
            output_path: Chemin de sauvegarde du rapport

        Returns:
            str: Contenu du rapport
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            if output_path is None:
                output_path = f"evaluation_report_{timestamp}.md"

            # G√©n√©rer le contenu du rapport
            report_content = f"""# üìä RAPPORT D'√âVALUATION MOD√àLE ADAN
## Date: {datetime.now().strftime("%d/%m/%Y %H:%M:%S")}

---

## üéØ **R√âSUM√â EX√âCUTIF**

Le mod√®le ADAN a √©t√© √©valu√© sur une p√©riode de test hors √©chantillon avec les r√©sultats suivants :

### üìà **M√âTRIQUES DE PERFORMANCE PRINCIPALES**

| M√©trique | Valeur | Seuil Cible | Status |
|----------|--------|-------------|---------|
| **Sharpe Ratio** | {self.performance_metrics["sharpe_ratio"]:.3f} | > 3.0 | {"‚úÖ" if self.performance_metrics["sharpe_ratio"] > 3.0 else "‚ö†Ô∏è"} |
| **Maximum Drawdown** | {self.performance_metrics["max_drawdown"]:.2%} | < 20% | {"‚úÖ" if self.performance_metrics["max_drawdown"] < 0.20 else "‚ö†Ô∏è"} |
| **Win Rate** | {self.performance_metrics["win_rate"]:.1f}% | > 48% | {"‚úÖ" if self.performance_metrics["win_rate"] > 48 else "‚ö†Ô∏è"} |
| **Profit Factor** | {self.performance_metrics["profit_factor"]:.2f} | > 1.5 | {"‚úÖ" if self.performance_metrics["profit_factor"] > 1.5 else "‚ö†Ô∏è"} |
| **Return Total** | {self.performance_metrics["total_return"]:.2%} | > 0% | {"‚úÖ" if self.performance_metrics["total_return"] > 0 else "‚ö†Ô∏è"} |

### üîç **M√âTRIQUES D√âTAILL√âES**

- **Volatilit√© Annualis√©e**: {self.performance_metrics["volatility"]:.2%}
- **Ratio de Calmar**: {self.performance_metrics["calmar_ratio"]:.3f}
- **Ratio de Sortino**: {self.performance_metrics["sortino_ratio"]:.3f}
- **Nombre Total de Trades**: {self.performance_metrics["total_trades"]}
- **Trades Gagnants**: {self.performance_metrics["winning_trades"]}
- **Trades Perdants**: {self.performance_metrics["losing_trades"]}
- **Trade Moyen**: {self.performance_metrics["average_trade"]:.4f}
- **Meilleur Trade**: {self.performance_metrics["best_trade"]:.4f}
- **Pire Trade**: {self.performance_metrics["worst_trade"]:.4f}

---

## üéõÔ∏è **VALIDATION TECHNIQUE**

### ‚úÖ **POIDS DE FUSION OPTIMAUX**
Les poids de fusion ont √©t√© valid√©s selon l'optimisation Optuna :
- Worker 0 (Conservative): 25.0%
- Worker 1 (Moderate): 27.0%
- Worker 2 (Aggressive): 30.0% ‚≠ê
- Worker 3 (Adaptive): 18.0%

### üèóÔ∏è **ARCHITECTURE MOD√àLE**
- **Algorithme**: PPO (Proximal Policy Optimization)
- **R√©seau**: CNN + LSTM avec attention
- **Fusion**: Ensemble pond√©r√© √† 4 workers
- **Export**: PyTorch (.zip) + Poids fusion (.pth)

---

## üìä **ANALYSE DE PERFORMANCE**

### üéØ **POINTS FORTS**
"""

            # Analyser les performances et ajouter les points forts/faibles
            if self.performance_metrics["sharpe_ratio"] > 3.0:
                report_content += "- ‚úÖ **Sharpe Ratio excellent** (> 3.0) - Rendement ajust√© au risque optimal\n"
            if self.performance_metrics["win_rate"] > 55:
                report_content += (
                    "- ‚úÖ **Taux de succ√®s √©lev√©** - Plus de 55% de trades gagnants\n"
                )
            if self.performance_metrics["max_drawdown"] < 0.15:
                report_content += (
                    "- ‚úÖ **Drawdown ma√Ætris√©** - Risque de perte contr√¥l√©\n"
                )
            if self.performance_metrics["profit_factor"] > 2.0:
                report_content += (
                    "- ‚úÖ **Profit Factor solide** - Gains sup√©rieurs aux pertes\n"
                )

            report_content += f"""
### ‚ö†Ô∏è **POINTS D'ATTENTION**
"""

            if self.performance_metrics["sharpe_ratio"] < 2.0:
                report_content += (
                    "- ‚ö†Ô∏è **Sharpe Ratio faible** - Rendement/risque √† am√©liorer\n"
                )
            if self.performance_metrics["volatility"] > 0.30:
                report_content += (
                    "- ‚ö†Ô∏è **Volatilit√© √©lev√©e** - Strat√©gie potentiellement risqu√©e\n"
                )
            if self.performance_metrics["max_drawdown"] > 0.25:
                report_content += (
                    "- ‚ö†Ô∏è **Drawdown important** - Risque de pertes significatives\n"
                )

            report_content += f"""
---

## üöÄ **RECOMMANDATIONS**

### ‚úÖ **PR√äT POUR D√âPLOIEMENT**
"""

            # √âvaluer si le mod√®le est pr√™t
            deployment_ready = (
                self.performance_metrics["sharpe_ratio"] > 2.0
                and self.performance_metrics["max_drawdown"] < 0.25
                and self.performance_metrics["win_rate"] > 45
                and self.performance_metrics["profit_factor"] > 1.2
            )

            if deployment_ready:
                report_content += """
- üéØ **Mod√®le VALID√â** pour d√©ploiement progressif
- üí∞ Commencer avec **position size 0.1%** du capital
- üìä Monitoring en temps r√©el requis
- ‚è±Ô∏è R√©√©valuation recommand√©e apr√®s 30 jours
"""
            else:
                report_content += """
- ‚ö†Ô∏è **Fine-tuning recommand√©** avant d√©ploiement
- üîß Optimiser les hyperparam√®tres
- üìà Am√©liorer le ratio rendement/risque
- üß™ Tests suppl√©mentaires requis
"""

            report_content += f"""
### üîß **OPTIMISATIONS FUTURES**
- **Ensemble Learning**: Tester d'autres combinaisons de poids
- **Feature Engineering**: Ajouter indicateurs techniques avanc√©s
- **Risk Management**: Impl√©menter stop-loss dynamique
- **Multi-Timeframe**: Fusion d'horizons temporels multiples

---

## üìã **M√âTADONN√âES**
- **Fichier Mod√®le**: `{os.path.basename(self.model_path)}`
- **Configuration**: `{self.config.get("name", "default")}`
- **P√©riode Test**: 3 mois hors √©chantillon
- **Timestamp**: {timestamp}
- **Version**: ADAN v2.0

---

## üèÜ **CONCLUSION**

{"‚úÖ **MOD√àLE APPROUV√â** - Performances satisfaisantes pour d√©ploiement" if deployment_ready else "‚ö†Ô∏è **OPTIMISATION REQUISE** - Performances √† am√©liorer avant d√©ploiement"}

Le syst√®me de trading ADAN montre des r√©sultats {"prometteurs" if deployment_ready else "mitig√©s"} avec un potentiel {"confirm√©" if deployment_ready else "√† d√©velopper"} pour le trading algorithmique automatis√©.

---
*Rapport g√©n√©r√© automatiquement par ADAN Evaluation System*
"""

            # Sauvegarder le rapport
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(report_content)

            self.logger.info(f"üìÑ Rapport sauvegard√©: {output_path}")
            return report_content

        except Exception as e:
            self.logger.error(f"‚ùå Erreur g√©n√©ration rapport: {e}")
            return ""

    def run_complete_evaluation(self) -> bool:
        """
        Lance l'√©valuation compl√®te du mod√®le.

        Returns:
            bool: True si √©valuation r√©ussie, False sinon
        """
        try:
            self.logger.info("üöÄ D√©marrage √©valuation compl√®te du mod√®le ADAN")

            # 1. Charger le mod√®le
            if not self.load_model():
                return False

            # 2. Valider les poids de fusion
            weights_valid = self.validate_fusion_weights()

            # 3. Configurer l'environnement de test
            if not self.setup_environment():
                return False

            # 4. Ex√©cuter le backtesting
            backtest_results = self.run_backtest()
            if not backtest_results:
                return False

            # 5. G√©n√©rer le rapport final
            report_path = "reports/final_evaluation_report.md"
            os.makedirs("reports", exist_ok=True)
            report = self.generate_report(report_path)

            # 6. Affichage r√©sum√© console
            self.logger.info("\n" + "=" * 60)
            self.logger.info("üèÜ √âVALUATION TERMIN√âE - R√âSULTATS FINAUX")
            self.logger.info("=" * 60)
            self.logger.info(
                f"üìä Sharpe Ratio: {self.performance_metrics['sharpe_ratio']:.3f}"
            )
            self.logger.info(
                f"üìâ Max Drawdown: {self.performance_metrics['max_drawdown']:.2%}"
            )
            self.logger.info(
                f"üéØ Win Rate: {self.performance_metrics['win_rate']:.1f}%"
            )
            self.logger.info(
                f"üí∞ Profit Factor: {self.performance_metrics['profit_factor']:.2f}"
            )
            self.logger.info(
                f"üìà Return Total: {self.performance_metrics['total_return']:.2%}"
            )
            self.logger.info(
                f"üéõÔ∏è Poids Fusion: {'‚úÖ Valid√©s' if weights_valid else '‚ö†Ô∏è √Ä corriger'}"
            )
            self.logger.info(f"üìÑ Rapport: {report_path}")
            self.logger.info("=" * 60)

            return True

        except Exception as e:
            self.logger.error(f"‚ùå Erreur √©valuation compl√®te: {e}")
            return False


def main():
    """Fonction principale."""
    parser = argparse.ArgumentParser(description="√âvaluation compl√®te du mod√®le ADAN")
    parser.add_argument(
        "--config", required=True, help="Chemin vers le fichier de configuration"
    )
    parser.add_argument("--model", required=True, help="Chemin vers le mod√®le entra√Æn√©")
    parser.add_argument(
        "--episodes", type=int, default=10, help="Nombre d'√©pisodes de test"
    )
    parser.add_argument("--output", help="Chemin de sauvegarde du rapport")

    args = parser.parse_args()

    # Configuration logging
    setup_logging()
    logger = logging.getLogger(__name__)

    try:
        logger.info("üöÄ D√©marrage √©valuation mod√®le ADAN")

        # V√©rifier que les fichiers existent
        if not os.path.exists(args.config):
            logger.error(f"‚ùå Configuration non trouv√©e: {args.config}")
            return False

        if not os.path.exists(args.model):
            logger.error(f"‚ùå Mod√®le non trouv√©: {args.model}")
            return False

        # Cr√©er et lancer l'√©valuateur
        evaluator = ModelEvaluator(args.config, args.model)
        success = evaluator.run_complete_evaluation()

        if success:
            logger.info("‚úÖ √âvaluation termin√©e avec succ√®s!")
            return True
        else:
            logger.error("‚ùå √âvaluation √©chou√©e")
            return False

    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        return False


if __name__ == "__main__":
    main()
