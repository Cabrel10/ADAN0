# -*- coding: utf-8 -*-

"""Script d'entra√Ænement parall√®le pour instances ADAN."""

# Standard Library Imports
import argparse
import json
import logging
import os
import sys
import time
import traceback
import uuid
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Union

# Third-Party Imports
import gymnasium as gym
import gymnasium.spaces as spaces
import numpy as np
import pandas as pd
import torch
import torch as th
import torch.nn as nn
import yaml
from rich.console import Console
try:
    from sb3_contrib import RecurrentPPO
except Exception as _e:
    RecurrentPPO = None  # Will raise at runtime if used without sb3-contrib installed
from stable_baselines3.common.callbacks import BaseCallback, CallbackList
from stable_baselines3.common.logger import configure as sb3_logger_configure
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.vec_env import (
    DummyVecEnv, SubprocVecEnv, VecNormalize
)

# Local Application Imports
from adan_trading_bot.common.custom_logger import setup_logging
from adan_trading_bot.environment.checkpoint_manager import CheckpointManager
from adan_trading_bot.environment.multi_asset_chunked_env import (
    MultiAssetChunkedEnv
)
from adan_trading_bot.training.trainer import (
    validate_environment,
    TimeoutManager,
    TimeoutException as TMTimeoutException,
)
from adan_trading_bot.agent.custom_recurrent_policy import CustomRecurrentPolicy




# ==============================================================================
# SECTION: Configuration et ex√©cution du script
# ==============================================================================
# Configure logging to be less verbose
logging.getLogger().setLevel(logging.ERROR)
for logger_name in [
    "root", "matplotlib", "numpy", "pandas", "torch",
    "stable_baselines3", "gymnasium", "gym", "adan_trading_bot"
]:
    logging.getLogger(logger_name).setLevel(logging.ERROR)

# Suppress all warnings for a cleaner output
warnings.filterwarnings("ignore")

# Initialize Rich Console
console = Console()

class HierarchicalTrainingCallback(BaseCallback):
    """Callback pour affichage hi√©rarchique de l'entra√Ænement avec m√©triques d√©taill√©es."""

    def __init__(
        self,
        verbose=1,
        display_freq=1000,
        total_timesteps=1000000,
        initial_capital=20.50,
    ):
        super().__init__(verbose)
        self.display_freq = display_freq
        self.total_timesteps = total_timesteps
        self.initial_capital = initial_capital
        self.correlation_id = str(uuid.uuid4())
        self.start_time = time.time()
        self.last_step_summary = 0
        self.episode_rewards = []
        self.episode_count = 0
        self.positions = {}
        self.metrics = {
            "sharpe": 0.0,
            "sortino": 0.0,
            "profit_factor": 0.0,
            "max_dd": 0.0,
            "cagr": 0.0,
            "win_rate": 0.0,
            "trades": 0,
        }

    def _on_training_start(self):
        """D√©marrage de l'entra√Ænement avec affichage de la configuration."""
        logger.info("‚ï≠" + "‚îÄ" * 60 + "‚ïÆ")
        logger.info("‚îÇ" + " " * 15 + "üöÄ D√âMARRAGE ADAN TRAINING" + " " * 15 + "‚îÇ")
        logger.info("‚ï∞" + "‚îÄ" * 60 + "‚ïØ")
        logger.info(f"[TRAINING START] Correlation ID: {self.correlation_id}")
        logger.info(f"[TRAINING START] Total timesteps: {self.total_timesteps:,}")
        logger.info(f"[TRAINING START] Capital initial: ${self.initial_capital:.2f}")

    def _on_step(self) -> bool:
        """Appel√© √† chaque √©tape pour mettre √† jour l'affichage."""
        if self.num_timesteps % self.display_freq == 0 and self.num_timesteps > 0:
            self._log_detailed_metrics()
        return True

    def _log_detailed_metrics(self):
        """Affichage d√©taill√© des m√©triques pour chaque worker (compatible SubprocVecEnv)."""
        try:
            logger.info("‚ï≠" + "‚îÄ" * 90 + "‚ïÆ")
            logger.info(
                "‚îÇ" + " " * 30 + f"√âTAPE {self.num_timesteps:,}" + " " * 30 + "‚îÇ"
            )
            logger.info("‚ï∞" + "‚îÄ" * 90 + "‚ïØ")

            self._display_model_metrics()

            infos = self.locals.get("infos")
            if isinstance(infos, list) and all(isinstance(i, dict) for i in infos):
                logger.info(f"üìä WORKERS ANALYSIS | Total: {len(infos)} workers")
                logger.info("=" * 92)
                for i, info in enumerate(infos):
                    final_info = info.get('final_info', info)
                    self._display_worker_summary(i, final_info)
            else:
                logger.info("Impossible de r√©cup√©rer les 'infos' des workers depuis self.locals.")

            elapsed = time.time() - self.start_time
            steps_per_sec = self.num_timesteps / elapsed if elapsed > 0 else 0
            logger.info("=" * 92)
            logger.info(
                f"‚è±Ô∏è  GLOBAL TIMING | Elapsed: {elapsed / 60:.1f}min | Speed: {steps_per_sec:.1f} steps/s"
            )
            logger.info("‚îÄ" * 92)

        except Exception as e:
            logger.error(f"Erreur lors de l'affichage des m√©triques: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

            self._display_model_metrics()

            # Utiliser self.locals['infos'] qui est fourni par SB3 √† chaque step
            # C'est la m√©thode la plus s√ªre pour le multiprocessing
            infos = self.locals.get("infos")
            if isinstance(infos, list) and all(isinstance(i, dict) for i in infos):
                logger.info(f"üìä WORKERS ANALYSIS | Total: {len(infos)} workers")
                logger.info("=" * 92)
                for i, info in enumerate(infos):
                    # L'info de l'environnement final (non-wrapp√©) est souvent sous la cl√© 'final_info'
                    final_info = info.get('final_info', info)
                    self._display_worker_summary(i, final_info)
            else:
                logger.info("Impossible de r√©cup√©rer les 'infos' des workers depuis self.locals.")

            elapsed = time.time() - self.start_time
            steps_per_sec = self.num_timesteps / elapsed if elapsed > 0 else 0
            logger.info("=" * 92)
            logger.info(
                f"‚è±Ô∏è  GLOBAL TIMING | Elapsed: {elapsed / 60:.1f}min | Speed: {steps_per_sec:.1f} steps/s"
            )
            logger.info("‚îÄ" * 92)

        except Exception as e:
            logger.error(f"Erreur lors de l'affichage des m√©triques: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

    def _display_individual_worker_metrics(self, worker_id: int, worker_env_wrapper):
        """Afficher les m√©triques d√©taill√©es d'un worker sp√©cifique."""
        try:
            # Naviguer √† travers les wrappers pour trouver l'environnement r√©el et ses m√©triques
            current_env = worker_env_wrapper
            metrics = None
            info = None

            # Essayer de trouver les m√©triques √† travers les couches de wrappers
            while hasattr(current_env, "env") or hasattr(
                current_env, "get_portfolio_metrics"
            ):
                if hasattr(current_env, "get_portfolio_metrics"):
                    try:
                        metrics = current_env.get_portfolio_metrics()
                        break
                    except:
                        pass

                if hasattr(current_env, "last_info"):
                    info = current_env.last_info

                current_env = getattr(current_env, "env", None)
                if current_env is None:
                    break

            # Utiliser info comme fallback
            if not metrics and info:
                metrics = info

            if metrics:
                self._display_worker_summary(worker_id, metrics)
            else:
                logger.info(
                    f"‚îÇ WORKER {worker_id} | ‚ùå Impossible de r√©cup√©rer les m√©triques."
                )

        except Exception as e:
            logger.error(
                f"Erreur lors de l'affichage des m√©triques du worker {worker_id}: {e}"
            )

    def _display_worker_summary(self, worker_id: int, metrics: dict):
        """Afficher le r√©sum√© complet des m√©triques d'un worker."""
        try:
            # M√©triques de base (avec fallback pour compatibilit√©)
            portfolio_value = metrics.get("total_value", metrics.get("portfolio_value", self.initial_capital))
            cash = metrics.get("cash", self.initial_capital)
            roi = (
                ((portfolio_value - self.initial_capital) / self.initial_capital) * 100
                if self.initial_capital > 0
                else 0
            )
            drawdown = metrics.get("drawdown", 0.0)
            max_dd = metrics.get("max_drawdown", metrics.get("max_dd", 0.0))
            sharpe = metrics.get("sharpe_ratio", metrics.get("sharpe", 0.0))
            win_rate = metrics.get("win_rate", 0.0)
            total_trades = metrics.get("total_trades", metrics.get("trades", 0))
            trade_attempts = metrics.get("trade_attempts")
            invalid_trade_attempts = metrics.get("invalid_trade_attempts")

            # M√©triques de trading d√©taill√©es
            valid_trades = metrics.get("valid_trades", 0)
            invalid_trades = (
                total_trades - valid_trades if total_trades > valid_trades else 0
            )
            current_positions = metrics.get("positions", {})
            closed_positions = metrics.get("closed_positions", [])

            # Informations de r√©compense et p√©nalit√©s
            last_reward = metrics.get("last_reward", 0.0)
            last_penalty = metrics.get("last_penalty", 0.0)
            cumulative_reward = metrics.get("cumulative_reward", 0.0)

            # Dates et actifs
            current_date = metrics.get("current_date", "N/A")
            active_assets = list(current_positions.keys()) if current_positions else []

            # En-t√™te du worker
            logger.info(
                f"‚ï≠‚îÄ‚îÄ‚îÄ WORKER {worker_id} ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ"
            )

            # Ligne 1: Portfolio et Performance
            logger.info(
                f"‚îÇ üìä PORTFOLIO  | Valeur: ${portfolio_value:>10.2f} | Cash: ${cash:>10.2f} | ROI: {roi:>+7.2f}% ‚îÇ"
            )

            # Ligne 2: Risk Management
            logger.info(
                f"‚îÇ ‚ö†Ô∏è  RISK      | Drawdown: {drawdown:>6.2f}% | Max DD: {max_dd:>6.2f}% | Sharpe: {sharpe:>6.2f}     ‚îÇ"
            )

            # Ligne 3: Trading Statistics
            logger.info(
                f"‚îÇ üìà TRADING    | Total: {total_trades:>3d} | Valid: {valid_trades:>3d} | Invalid: {invalid_trades:>3d} | Win Rate: {win_rate:>5.1f}% ‚îÇ"
            )

            # Ligne 3b: Actions de l'agent
            if trade_attempts is not None and invalid_trade_attempts is not None:
                invalid_rate = (invalid_trade_attempts / trade_attempts * 100) if trade_attempts > 0 else 0
                logger.info(
                    f"‚îÇ üéØ ACTIONS    | Tentatives: {trade_attempts:>4d} | Invalides: {invalid_trade_attempts:>4d} ({invalid_rate:5.1f}%)             ‚îÇ"
                )

            # Ligne 4: Rewards & Penalties
            logger.info(
                f"‚îÇ üéØ REWARDS    | Last: {last_reward:>+8.4f} | Penalty: {last_penalty:>+8.4f} | Cumul: {cumulative_reward:>+8.2f}   ‚îÇ"
            )

            # Ligne 5: Temporal & Assets
            date_str = str(current_date)[:10] if current_date != "N/A" else "N/A"
            assets_str = ", ".join(active_assets[:3]) if active_assets else "Aucun"
            if len(active_assets) > 3:
                assets_str += f"+{len(active_assets) - 3}"
            logger.info(
                f"‚îÇ üìÖ CONTEXT    | Date: {date_str:>10s} | Active Assets: {assets_str:<25s}              ‚îÇ"
            )

            # Positions ouvertes d√©taill√©es (si pr√©sentes)
            if current_positions:
                logger.info("‚îÇ ‚îú‚îÄ POSITIONS OUVERTES:" + " " * 54 + "‚îÇ")
                for asset, pos in list(current_positions.items())[
                    :3
                ]:  # Max 3 pour l'affichage
                    if isinstance(pos, dict):
                        size = pos.get("size", 0)
                        entry_price = pos.get("entry_price", 0)
                        current_value = pos.get("value", 0)
                        pnl = pos.get("unrealized_pnl", 0)
                        logger.info(
                            f"‚îÇ ‚îÇ  {asset:<8s} | Size: {size:>6.2f} @ {entry_price:>8.4f} | Val: ${current_value:>7.2f} | PnL: {pnl:>+6.2f} ‚îÇ"
                        )

                remaining = len(current_positions) - 3
                if remaining > 0:
                    logger.info(
                        f"‚îÇ ‚îÇ  ... et {remaining} autres positions" + " " * 44 + "‚îÇ"
                    )

            # Derniers trades ferm√©s (si disponibles)
            if closed_positions:
                recent_closed = (
                    closed_positions[-2:]
                    if len(closed_positions) >= 2
                    else closed_positions
                )
                logger.info("‚îÇ ‚îú‚îÄ DERNIERS TRADES FERM√âS:" + " " * 48 + "‚îÇ")
                for trade in recent_closed:
                    if isinstance(trade, dict):
                        asset = trade.get("asset", "N/A")
                        profit = trade.get("profit", 0)
                        duration = trade.get("duration", "N/A")
                        close_reason = trade.get("reason", "N/A")[:8]
                        logger.info(
                            f"‚îÇ ‚îÇ  {asset:<8s} | Profit: {profit:>+8.2f} | Dur√©e: {str(duration):<6s} | Raison: {close_reason:<8s}  ‚îÇ"
                        )

            logger.info(
                f"‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ"
            )

        except Exception as e:
            logger.error(
                f"Erreur lors de l'affichage des m√©triques du worker {worker_id}: {e}"
            )
            logger.info(f"‚îÇ WORKER {worker_id} | ‚ùå Erreur d'affichage des m√©triques.")

    def _display_model_metrics(self):
        """Afficher les m√©triques globales du mod√®le PPO."""
        try:
            # R√©cup√©rer les m√©triques du mod√®le PPO
            model_metrics = {}
            total_loss = 0.0
            policy_loss = 0.0
            value_loss = 0.0
            entropy = 0.0

            if hasattr(self.model, "logger") and hasattr(
                self.model.logger, "name_to_value"
            ):
                model_metrics = self.model.logger.name_to_value
                total_loss = model_metrics.get("train/loss", 0.0)
                policy_loss = model_metrics.get("train/policy_loss", 0.0)
                value_loss = model_metrics.get("train/value_loss", 0.0)
                entropy = model_metrics.get("train/entropy_loss", 0.0)

            # Model Learning Metrics
            logger.info(
                f"üß† MODEL | Loss: {total_loss:.4f} | Policy: {policy_loss:+.4f} | "
                f"Value: {value_loss:.4f} | Entropy: {entropy:.4f}"
            )

        except Exception as e:
            logger.error(f"Erreur lors de l'affichage des m√©triques du mod√®le: {e}")

    def _on_rollout_end(self):
        """Appel√© √† la fin de chaque rollout pour capturer les positions ferm√©es."""
        try:
            # Essayer de r√©cup√©rer les positions ferm√©es via les wrappers am√©lior√©s
            if hasattr(self.model, "get_env"):
                env = self.model.get_env()
                closed_positions = []

                try:
                    # Si c'est un environnement vectoris√©, essayer d'acc√©der au premier environnement
                    if hasattr(env, "envs") and len(env.envs) > 0:
                        first_env = env.envs[0]
                        # Naviguer √† travers les wrappers pour trouver notre GymnasiumToGymWrapper
                        current_env = first_env
                        while hasattr(current_env, "env"):
                            if isinstance(current_env, GymnasiumToGymWrapper):
                                metrics = current_env.get_portfolio_metrics()
                                closed_positions = metrics.get("closed_positions", [])
                                break
                            current_env = (
                                current_env.env if hasattr(current_env, "env") else None
                            )
                            if current_env is None:
                                break

                    # Note: ne pas utiliser get_attr("last_info") en mode Subproc (non picklable / attribut absent)

                    if closed_positions:
                        logger.info(
                            "‚ï≠" + "‚îÄ" * 25 + " Positions Ferm√©es " + "‚îÄ" * 25 + "‚ïÆ"
                        )
                        for pos in closed_positions:
                            if isinstance(pos, dict):
                                asset = pos.get("asset", "Unknown")
                                size = pos.get("size", 0)
                                entry_price = pos.get("entry_price", 0)
                                exit_price = pos.get("exit_price", 0)
                                pnl = pos.get("pnl", 0)
                                pnl_pct = pos.get("pnl_pct", 0)
                                logger.info(
                                    f"‚îÇ {asset}: Taille: {size:.2f} | Entr√©e: {entry_price:.4f} | "
                                    f"Sortie: {exit_price:.4f} | PnL: ${pnl:.2f} ({pnl_pct:.2f}%)"
                                    + " " * 5
                                    + "‚îÇ"
                                )
                        logger.info("‚ï∞" + "‚îÄ" * 68 + "‚ïØ")
                except Exception as e:
                    logger.debug(f"Impossible de r√©cup√©rer les positions ferm√©es: {e}")
        except Exception as e:
            logger.error(f"Erreur lors du traitement des positions ferm√©es: {e}")

    def _on_training_end(self):
        """Fin de l'entra√Ænement avec r√©sum√© complet."""
        elapsed = time.time() - self.start_time
        logger.info("‚ï≠" + "‚îÄ" * 60 + "‚ïÆ")
        logger.info("‚îÇ" + " " * 15 + "‚úÖ ENTRA√éNEMENT TERMIN√â" + " " * 15 + "‚îÇ")
        logger.info("‚ï∞" + "‚îÄ" * 60 + "‚ïØ")
        logger.info(f"[TRAINING END] Total steps: {self.num_timesteps:,}")
        logger.info(f"[TRAINING END] Duration: {elapsed / 60:.1f} minutes")
        logger.info(f"[TRAINING END] Episodes: {self.episode_count}")

        # R√©sum√© final des performances
        if self.episode_rewards:
            final_reward = (
                np.mean(self.episode_rewards[-10:])
                if len(self.episode_rewards) >= 10
                else np.mean(self.episode_rewards)
            )
            logger.info(f"[TRAINING END] Final Mean Reward: {final_reward:.2f}")

        logger.info(f"[TRAINING END] Correlation ID: {self.correlation_id}")

class WorkerMetricsLogger(BaseCallback):
    """Journalise les m√©triques par worker vers TensorBoard et JSONL."""

    def __init__(
        self,
        log_dir: str,
        initial_balance: float,
        log_interval: int = 500,
        tensorboard_prefix: str = "workers",
    ) -> None:
        super().__init__(verbose=0)
        self.log_dir = log_dir
        self.initial_balance = float(initial_balance)
        self.log_interval = max(1, int(log_interval))
        self.tensorboard_prefix = tensorboard_prefix.rstrip("/")
        self.log_path: Optional[str] = None
        self._log_file = None
        self._last_worker_step: Dict[str, int] = {}

    def _ensure_log_file(self) -> None:
        if self._log_file is None and self.log_path:
            self._log_file = open(self.log_path, "a", encoding="utf-8")

    def _close_log_file(self) -> None:
        if self._log_file:
            try:
                self._log_file.close()
            finally:
                self._log_file = None

    def _unwrap_envs(self):
        env = getattr(self, "training_env", None)
        if env is None:
            return []
        current = env
        visited = set()
        while current is not None and id(current) not in visited:
            visited.add(id(current))
            if hasattr(current, "envs"):
                return getattr(current, "envs", [])
            if hasattr(current, "venv"):
                current = getattr(current, "venv")
                continue
            if hasattr(current, "env"):
                current = getattr(current, "env")
                continue
            break
        return []

    def _extract_metrics(self, env) -> Optional[Dict[str, Any]]:
        depth = 0
        current = env
        while current is not None and depth < 10:
            if hasattr(current, "get_portfolio_metrics"):
                try:
                    metrics = current.get_portfolio_metrics()
                    if isinstance(metrics, dict):
                        return metrics
                except Exception:
                    pass
            current = getattr(current, "env", None)
            depth += 1
        return None

    @staticmethod
    def _extract_worker_id(env, fallback: int) -> int:
        depth = 0
        current = env
        while current is not None and depth < 10:
            worker_id = getattr(current, "worker_id", None)
            if worker_id is not None:
                try:
                    return int(worker_id)
                except (TypeError, ValueError):
                    return fallback
            current = getattr(current, "env", None)
            depth += 1
        return fallback

    @staticmethod
    def _to_float(value: Any, default: float = 0.0) -> float:
        try:
            return float(value)
        except (TypeError, ValueError):
            return float(default)

    @staticmethod
    def _to_int(value: Any, default: int = 0) -> int:
        try:
            return int(value)
        except (TypeError, ValueError):
            return int(default)

    @staticmethod
    def _sanitize(obj: Any) -> Any:
        if isinstance(obj, dict):
            return {k: WorkerMetricsLogger._sanitize(v) for k, v in obj.items()}
        if isinstance(obj, list):
            return [WorkerMetricsLogger._sanitize(v) for v in obj]
        if isinstance(obj, tuple):
            return [WorkerMetricsLogger._sanitize(v) for v in obj]
        if isinstance(obj, np.generic):
            return obj.item()
        return obj

    def _write_record(self, record: Dict[str, Any]) -> None:
        if not self._log_file:
            return
        try:
            self._log_file.write(json.dumps(record, ensure_ascii=False) + "\n")
            self._log_file.flush()
        except Exception as exc:
            logger.error("[WorkerMetricsLogger] √âchec d'√©criture JSON: %s", exc)

    def _log_tensorboard(
        self,
        worker_label: str,
        portfolio_value: float,
        penalty: float,
        inference_count: int,
        drawdown_pct: float,
        sharpe_ratio: float,
        trades: int,
    ) -> None:
        if self.logger is None:
            return
        base = f"{self.tensorboard_prefix}/{worker_label}"
        self.logger.record(f"{base}/capital", portfolio_value)
        self.logger.record(f"{base}/penalty", penalty)
        self.logger.record(f"{base}/inference_count", float(inference_count))
        self.logger.record(f"{base}/drawdown_pct", drawdown_pct)
        self.logger.record(f"{base}/sharpe", sharpe_ratio)
        self.logger.record(f"{base}/trades", float(trades))

    def _on_training_start(self) -> None:
        os.makedirs(self.log_dir, exist_ok=True)
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        self.log_path = os.path.join(
            self.log_dir, f"parallel_training_results_{timestamp}.jsonl"
        )
        self._ensure_log_file()

    def _on_step(self) -> bool:
        if self.num_timesteps % self.log_interval != 0:
            return True
        envs = self._unwrap_envs()
        if not envs:
            return True
        timestamp = datetime.utcnow().isoformat()
        total_workers = len(envs)

        for idx, worker_env in enumerate(envs):
            metrics = self._extract_metrics(worker_env)
            if not metrics:
                continue
            worker_id = self._extract_worker_id(worker_env, idx)
            worker_label = f"worker_{worker_id}"

            current_step = self._to_int(metrics.get("step", self.num_timesteps), self.num_timesteps)
            if self._last_worker_step.get(worker_label) == current_step:
                continue
            self._last_worker_step[worker_label] = current_step

            reward_components = metrics.get("reward_components", {}) or {}
            penalty = self._to_float(
                reward_components.get("frequency_penalty"), metrics.get("last_penalty", 0.0)
            )
            portfolio_value = self._to_float(metrics.get("portfolio_value"), self.initial_balance)
            cash = self._to_float(metrics.get("cash"), self.initial_balance)
            drawdown_pct = self._to_float(metrics.get("drawdown"))
            sharpe_ratio = self._to_float(metrics.get("sharpe"))
            trades = self._to_int(metrics.get("trades", 0))
            positions = self._to_int(metrics.get("num_positions", 0))
            inference_count = self._to_int(metrics.get("step", self.num_timesteps))
            action_stats = metrics.get("action_stats", {}) or {}

            record = {
                "timestamp": timestamp,
                "instance_id": worker_label,
                "worker_id": worker_id,
                "step": current_step,
                "episode": self._to_int(metrics.get("chunk", 0)),
                "reward": self._to_float(metrics.get("last_reward", 0.0)),
                "total_reward": self._to_float(metrics.get("cumulative_reward", 0.0)),
                "penalty": penalty,
                "positions": positions,
                "trades": trades,
                "parallel": {"workers": total_workers, "active": total_workers},
                "inference": {
                    "count": inference_count,
                    "action_mean": self._to_float(action_stats.get("action_mean")),
                    "action_std": self._to_float(action_stats.get("action_std")),
                    "action_min": self._to_float(action_stats.get("action_min")),
                    "action_max": self._to_float(action_stats.get("action_max")),
                },
                "portfolio": {
                    "portfolio_value": portfolio_value,
                    "cash": cash,
                    "drawdown": drawdown_pct,
                    "max_drawdown": self._to_float(metrics.get("max_dd")),
                    "sharpe": sharpe_ratio,
                    "sortino": self._to_float(metrics.get("sortino")),
                    "win_rate": self._to_float(metrics.get("win_rate")),
                    "leverage": self._to_float(metrics.get("leverage")),
                },
                "risk": self._sanitize(metrics.get("risk_metrics", {})),
                "initial_balance": self.initial_balance,
            }

            self._write_record(record)
            self._log_tensorboard(
                worker_label,
                portfolio_value,
                penalty,
                inference_count,
                drawdown_pct,
                sharpe_ratio,
                trades,
            )

        return True

    def _on_training_end(self) -> None:
        self._close_log_file()


# Timeout and environment validation
from adan_trading_bot.utils.timeout_manager import (
    TimeoutManager,
    TimeoutException as TMTimeoutException,
)
from adan_trading_bot.training.trainer import validate_environment

# Configuration du logger de base
from adan_trading_bot.common.custom_logger import setup_logging


def resolve_config_variables(config):
    """
    R√©sout les variables de configuration de type ${variable.path}.
    Version silencieuse optimis√©e sans logs debug.
    """
    import copy

    # Utiliser des chemins fixes pour √©viter les probl√®mes de r√©solution
    base_dir = "/home/morningstar/Documents/trading/bot"

    # Dictionnaire de substitution complet
    substitutions = {
        "${paths.base_dir}": base_dir,
        "${paths.data_dir}": f"{base_dir}/data",
        "${paths.raw_data_dir}": f"{base_dir}/data/raw",
        "${paths.processed_data_dir}": f"{base_dir}/data/processed",
        "${paths.indicators_data_dir}": f"{base_dir}/data/processed/indicators",
        "${paths.final_data_dir}": f"{base_dir}/data/final",
        "${paths.models_dir}": f"{base_dir}/models",
        "${paths.trained_models_dir}": f"{base_dir}/models/rl_agents",
        "${paths.logs_dir}": f"{base_dir}/logs",
        "${paths.reports_dir}": f"{base_dir}/reports",
        "${paths.figures_dir}": f"{base_dir}/reports/figures",
        "${paths.metrics_dir}": f"{base_dir}/reports/metrics",
        "${data.data_dirs.base}": f"{base_dir}/data/processed/indicators",
    }

    def simple_resolve(obj):
        if isinstance(obj, str):
            result = obj
            for var, value in substitutions.items():
                result = result.replace(var, value)
            return result
        elif isinstance(obj, dict):
            return {k: simple_resolve(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [simple_resolve(item) for item in obj]
        else:
            return obj

    return simple_resolve(copy.deepcopy(config))


def clean_worker_id(worker_id):
    """
    Nettoie l'ID du worker pour √©viter les erreurs JSONL.
    Convertit 'w0' en 0, 'w1' en 1, etc.

    Args:
        worker_id: ID du worker (peut √™tre string ou int)

    Returns:
        int: ID du worker nettoy√©
    """
    if isinstance(worker_id, str):
        # Supprimer le pr√©fixe 'w' si pr√©sent
        if worker_id.startswith("w"):
            try:
                return int(worker_id[1:])
            except ValueError:
                return 0
        # Essayer de convertir directement en int
        try:
            return int(worker_id)
        except ValueError:
            return 0
    elif isinstance(worker_id, int):
        return worker_id
    else:
        return 0


def log_worker_comparison(envs, n_workers):
    """
    Log comparison metrics between workers for debugging and analysis.

    Args:
        envs: List or VecEnv containing environment instances
        n_workers: Number of workers to compare
    """
    try:
        logger.info("=" * 80)
        logger.info("[WORKER COMPARISON] Performance Analysis")
        logger.info("=" * 80)

        for worker_id in range(n_workers):
            try:
                # Get environment instance
                if hasattr(envs, "get_attr"):
                    # VecEnv case
                    worker_env = (
                        envs.get_attr("envs")[0][worker_id]
                        if hasattr(envs.get_attr("envs")[0], "__getitem__")
                        else None
                    )
                elif isinstance(envs, list):
                    # List of envs
                    worker_env = envs[worker_id] if worker_id < len(envs) else None
                else:
                    worker_env = None

                if worker_env is None:
                    logger.warning(
                        f"[COMPARISON] Worker {worker_id}: Unable to access environment"
                    )
                    continue

                # Get performance metrics
                if hasattr(worker_env, "portfolio_manager") and hasattr(
                    worker_env.portfolio_manager, "metrics"
                ):
                    metrics = (
                        worker_env.portfolio_manager.metrics.calculate_metrics()
                        if hasattr(
                            worker_env.portfolio_manager.metrics, "calculate_metrics"
                        )
                        else {}
                    )
                    equity = (
                        worker_env.portfolio_manager.get_equity()
                        if hasattr(worker_env.portfolio_manager, "get_equity")
                        else 0.0
                    )
                    positions_count = getattr(worker_env, "positions_count", {})

                    logger.info(
                        f"[COMPARISON Worker {worker_id}] "
                        f"Trades: {metrics.get('total_trades', 0)}, "
                        f"Winrate: {metrics.get('winrate', 0.0):.1f}%, "
                        f"Equity: {equity:.2f} USDT, "
                        f"Counts: {positions_count}"
                    )
                else:
                    logger.info(f"[COMPARISON Worker {worker_id}] No metrics available")

            except Exception as e:
                logger.warning(
                    f"[COMPARISON Worker {worker_id}] Error accessing metrics: {e}"
                )

        logger.info("=" * 80)

    except Exception as e:
        logger.error(f"Error in worker comparison: {e}")


# Configurer le logger avec la configuration personnalis√©e
logger = setup_logging(
    default_level=logging.INFO,
    enable_console_logs=True,
    enable_json_logs=False,  # D√©sactiver les logs JSON par d√©faut
    force_plain_console=True,  # Forcer un affichage simple de la console
)

# D√©sactiver les avertissements sp√©cifiques
warnings.filterwarnings(
    action="ignore", category=UserWarning, module="stable_baselines3"
)
warnings.filterwarnings(action="ignore", category=FutureWarning)


class GymnasiumToGymWrapper(gym.Wrapper):
    """
    Wrapper minimal pour adapter un env Gymnasium (reset -> (obs,info), step -> (obs,rew,term,trunc,info))
    √† l'API Gym attendue par certains composants (SB3 DummyVecEnv / Monitor).
    """

    def __init__(self, env, rank=0):
        """Initialise le wrapper avec un rank pour √©viter les logs dupliqu√©s."""
        super().__init__(env)
        self.rank = rank
        self.log_prefix = f"[WORKER-{rank}]"
        self.last_info = {}
        self.last_obs = None
        self.episode_rewards = []
        self.episode_count = 0

    def reset(self, *, seed=None, options=None):
        """Garantit que reset() retourne toujours un tuple (obs, info)."""
        reset_result = super().reset(seed=seed, options=options)
        if isinstance(reset_result, tuple) and len(reset_result) == 2:
            obs, info = reset_result
            return obs, info
        else:
            return reset_result, {}

    def get_metrics(self):
        """Retourne les m√©triques actuelles de l'environnement."""
        return {
            "last_info": self.last_info,
            "episode_count": getattr(self, "episode_count", 0),
            "episode_rewards": getattr(self, "episode_rewards", []),
            "last_obs": self.last_obs,
        }

    def get_portfolio_metrics(self):
        """Retourne sp√©cifiquement les m√©triques de portfolio."""
        if hasattr(self, "last_info") and self.last_info:
            return {
                "portfolio_value": self.last_info.get("portfolio_value", 0),
                "cash": self.last_info.get("cash", 0),
                "drawdown": self.last_info.get("drawdown", 0),
                "positions": self.last_info.get("positions", {}),
                "closed_positions": self.last_info.get("closed_positions", []),
                "sharpe": self.last_info.get("sharpe", 0),
                "sortino": self.last_info.get("sortino", 0),
                "profit_factor": self.last_info.get("profit_factor", 0),
                "max_dd": self.last_info.get("max_dd", 0),
                "cagr": self.last_info.get("cagr", 0),
                "win_rate": self.last_info.get("win_rate", 0),
                "trades": self.last_info.get("trades", 0),
            }
        return {}

    def step(self, action):
        """Convertit le retour de step() de gymnasium (5 valeurs) au format SB3."""
        obs, reward, terminated, truncated, info = super().step(action)
        self.last_info = info.copy() if isinstance(info, dict) else {}
        self.last_obs = obs
        return obs, float(reward), terminated, truncated, info


# Gestion des exceptions
class TimeoutException(Exception):
    """Exception lev√©e quand le timeout est atteint (legacy)."""

    pass


# Import local
from adan_trading_bot.environment.checkpoint_manager import CheckpointManager

# Import local
from adan_trading_bot.utils.caching_utils import DataCacheManager

# Configuration du logger
logger = logging.getLogger(__name__)

# D√©finir le r√©pertoire racine du projet
PROJECT_ROOT = os.path.dirname(
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)
sys.path.append(PROJECT_ROOT)

from stable_baselines3.common.torch_layers import BaseFeaturesExtractor


class CustomCombinedExtractor(BaseFeaturesExtractor):
    """
    Extrait les caract√©ristiques √† partir des observations de type Dict.
    H√©rite de BaseFeaturesExtractor pour une meilleure int√©gration avec SB3.
    """

    def __init__(
        self,
        observation_space: spaces.Dict,
        features_dim: int = 256,  # Dimension de sortie requise par SB3
        cnn_output_dim: int = 64,
        mlp_extractor_net_arch: Optional[List[int]] = None,
    ) -> None:
        # Appel au constructeur parent avec la dimension de sortie
        super().__init__(observation_space, features_dim=features_dim)

        extractors = {}
        total_concat_size = 0

        # Pour chaque cl√© de l'espace d'observation
        for key, subspace in observation_space.spaces.items():
            if key == "observation":  # Traitement des donn√©es d'image
                # Calcul de la taille apr√®s aplatissement
                n_flatten = 1
                for i in range(len(subspace.shape)):
                    n_flatten *= subspace.shape[i]

                # R√©seau pour traiter les donn√©es d'image
                extractors[key] = nn.Sequential(
                    nn.Flatten(),
                    nn.Linear(n_flatten, 128),
                    nn.ReLU(),
                    nn.Linear(128, 128),
                    nn.ReLU(),
                )
                total_concat_size += 128
            else:  # Traitement des donn√©es vectorielles
                # Utilisation d'un MLP simple pour les donn√©es vectorielles
                extractors[key] = nn.Sequential(
                    nn.Linear(subspace.shape[0], 64),
                    nn.ReLU(),
                    nn.Linear(64, 32),
                    nn.ReLU(),
                )
                total_concat_size += 32

        self.extractors = nn.ModuleDict(extractors)

        # Couche lin√©aire finale pour adapter √† la dimension de sortie souhait√©e
        self.fc = nn.Sequential(nn.Linear(total_concat_size, features_dim), nn.ReLU())

    def forward(self, observations: Dict[str, th.Tensor]) -> th.Tensor:
        encoded_tensor_list = []

        # Extraire les caract√©ristiques pour chaque cl√©
        for key, extractor in self.extractors.items():
            if key in observations:
                # S'assurer que les observations sont au bon format
                x = observations[key]
                if isinstance(x, np.ndarray):
                    x = th.as_tensor(x, device=self.device, dtype=th.float32)
                encoded_tensor_list.append(extractor(x))

        # Concat√©ner toutes les caract√©ristiques et appliquer la couche finale
        return self.fc(th.cat(encoded_tensor_list, dim=1))


class GymnasiumToSB3Wrapper(gym.Wrapper):
    """Wrapper pour convertir un environnement Gymnasium en un format compatible avec Stable Baselines 3."""

    def __init__(self, env: gym.Env) -> None:
        """Initialize the Gymnasium to SB3 wrapper.

        Args:
            env: The Gymnasium environment to wrap
        """
        super().__init__(env)

        # Convertir l'espace d'observation en un format compatible
        if isinstance(env.observation_space, gym.spaces.Dict):
            # Pour les espaces de type Dict, on conserve la structure
            self.observation_space = env.observation_space
        else:
            # Pour les autres types d'espaces, on essaie de les convertir
            self.observation_space = env.observation_space

        self.action_space = env.action_space
        self.metadata = getattr(env, "metadata", {"render_modes": []})

        # Activer le mode vectoris√© si n√©cessaire
        self.is_vector_env = hasattr(env, "num_envs")

    def reset(self, **kwargs):
        """Reset the environment and return the initial observation and info."""
        obs, info = self.env.reset(**kwargs)

        # S'assurer que l'observation est au bon format
        if isinstance(obs, dict) and "observation" in obs and "portfolio_state" in obs:
            # D√©j√† au bon format
            return obs, info

        # G√©rer les autres formats d'observation si n√©cessaire
        return obs, info

    def step(self, action):
        """Take an action in the environment."""
        obs, reward, terminated, truncated, info = self.env.step(action)

        # S'assurer que l'observation est au bon format
        if isinstance(obs, dict) and "observation" in obs and "portfolio_state" in obs:
            # D√©j√† au bon format
            return obs, reward, terminated or truncated, False, info

        # G√©rer les autres formats d'observation si n√©cessaire
        return obs, reward, terminated or truncated, False, info

    def render(self, mode: str = "human"):
        return self.env.render(mode)


# Local application imports
from adan_trading_bot.environment.multi_asset_chunked_env import MultiAssetChunkedEnv
from adan_trading_bot.environment.dynamic_behavior_engine import DynamicBehaviorEngine

# Import d√©j√† effectu√© plus haut
from stable_baselines3.common.vec_env import (
    DummyVecEnv,
    SubprocVecEnv,
    VecNormalize,
    VecTransposeImage,
    VecEnv,
)
from stable_baselines3.common.utils import set_random_seed

# SOLUTION IMMORTALIT√â ADAN: Registre global des DBE pour survivre aux recr√©ations d'environnement
_GLOBAL_DBE_REGISTRY = {}


def _normalize_obs_for_sb3(obs: Any) -> Union[np.ndarray, Dict[str, np.ndarray]]:
    """Normalize observation for Stable Baselines 3 compatibility.

    Handles Gym's tuple (obs, info) and ensures proper numpy arrays.
    Converts observations to the format expected by SB3.

    Args:
        obs: Observation to normalize (can be tuple, dict, numpy array).
            L'observation √† normaliser (peut √™tre un tuple, un dict,
            un tableau numpy).

    Returns:
        Union[np.ndarray, Dict[str, np.ndarray]]: Normalized observation for SB3.
    """
    # Handle case where obs is a tuple (obs, info) from Gymnasium
    if isinstance(obs, tuple) and len(obs) >= 1:
        obs = obs[0]  # Take only the observation part

    # Handle dict observations (for MultiInputPolicy)
    if isinstance(obs, dict):
        # Ensure all values are numpy arrays and handle potential tuples
        normalized_obs = {}
        for k, v in obs.items():
            if isinstance(v, tuple):
                v = v[0]  # Take first element if it's a tuple
            normalized_obs[k] = np.asarray(v, dtype=np.float32)
        return normalized_obs

    # Handle numpy arrays
    if isinstance(obs, np.ndarray):
        return obs.astype(np.float32, copy=False)

    # Handle PyTorch tensors
    if hasattr(obs, "numpy"):
        return obs.detach().cpu().numpy()

    # Handle lists and other array-like objects
    try:
        return np.asarray(obs, dtype=np.float32)
    except Exception as e:
        error_msg = f"Could not convert observation to numpy array: {obs}, error: {e}"
        raise ValueError(error_msg) from e


class ResetObsAdapter:
    """
    Adapter minimal pour rendre un env compatible avec Stable-Baselines3 (Gym API).
    - Si env.reset() renvoie (obs, info) (Gymnasium), on renvoie obs uniquement.
    - Si env.step() renvoie 5 √©l√©ments (obs, reward, terminated, truncated, info),
      on convertit en (obs, reward, done, info) o√π done = terminated or truncated.
    Utiliser : env = ResetObsAdapter(env)
    """

    def __init__(self, env):
        self.env = env
        self.observation_space = getattr(env, "observation_space", None)
        self.action_space = getattr(env, "action_space", None)
        self.metadata = getattr(env, "metadata", {})
        # Propri√©t√© unwrapped pour la compatibilit√© avec SB3
        self.unwrapped = env.unwrapped if hasattr(env, "unwrapped") else env

    def reset(self, **kwargs):
        # Appel √† la m√©thode reset de l'environnement sous-jacent
        reset_result = self.env.reset(**kwargs)

        # Journalisation pour le d√©bogage
        logger.debug(f"ResetObsAdapter.reset - Type de sortie: {type(reset_result)}")
        if isinstance(reset_result, tuple):
            logger.debug(
                f"ResetObsAdapter.reset - Longueur du tuple: {len(reset_result)}"
            )

        # G√©rer le cas o√π l'environnement retourne un tuple (obs, info)
        if isinstance(reset_result, tuple) and len(reset_result) == 2:
            obs, info = reset_result
            logger.debug(
                "ResetObsAdapter.reset - Tuple (obs, info) d√©tect√©, retourne obs uniquement"
            )

            # V√©rifier que l'observation est dans le bon format
            if not isinstance(obs, (np.ndarray, dict)):
                logger.warning(
                    f"ResetObsAdapter.reset - Type d'observation inattendu: {type(obs)}"
                )

            return obs

        # Si ce n'est pas un tuple, retourner tel quel
        logger.debug("ResetObsAdapter.reset - Sortie simple d√©tect√©e")
        return reset_result

    def step(self, action):
        out = self.env.step(action)
        # gymnasium step: (obs, reward, terminated, truncated, info)
        if isinstance(out, tuple) and len(out) == 5:
            obs, reward, terminated, truncated, info = out
            done = bool(terminated or truncated)
            return obs, reward, done, info
        return out

    def render(self, *args, **kwargs):
        return getattr(self.env, "render")(*args, **kwargs)

    def close(self):
        return getattr(self.env, "close")()

    def seed(self, seed=None):
        if hasattr(self.env, "seed"):
            return self.env.seed(seed)
        return None


class TrainingProgressCallback(BaseCallback):
    """
    A custom callback that prints a detailed training progress table.
    Displays training metrics, performance, portfolio value, and learning stats.
    """

    def __init__(
        self,
        check_freq: int = 1000,  # Check every N time steps
        verbose: int = 1,
    ):
        super(TrainingProgressCallback, self).__init__(verbose)
        self.check_freq = check_freq
        self.start_time = time.time()
        self.last_time = self.start_time
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_times = []
        self.total_steps = 0
        self.last_log_steps = 0

    def _on_training_start(self) -> None:
        """Print the header when training starts."""
        self.start_time = time.time()
        self.last_time = self.start_time
        print("\n" + "=" * 100)
        print("D√âMARRAGE DE L'ENTRA√éNEMENT")
        print("=" * 100)
        self._print_header()

    def _on_step(self) -> bool:
        """Called at each environment step."""
        # Update counters
        self.total_steps = self.num_timesteps

        # Log every check_freq steps
        if self.n_calls % self.check_freq == 0:
            self._log_progress()

        return True

    def _on_rollout_end(self) -> None:
        """Called at the end of each rollout."""
        # Get rollout information
        if len(self.model.ep_info_buffer) > 0 and len(self.model.ep_info_buffer[0]) > 0:
            ep_info = self.model.ep_info_buffer[0][0]  # Get first env's first episode
            if "r" in ep_info and "l" in ep_info:
                self.episode_rewards.append(ep_info["r"])
                self.episode_lengths.append(ep_info["l"])
                self.episode_times.append(time.time() - self.last_time)
                self.last_time = time.time()

    def _log_progress(self) -> None:
        """Log training progress with detailed metrics."""
        # Calculate metrics
        current_time = time.time()
        elapsed_time = current_time - self.start_time
        steps_per_sec = (
            (self.total_steps - self.last_log_steps) / (current_time - self.last_time)
            if current_time > self.last_time
            else 0
        )

        # Get environment statistics
        if hasattr(self.model.get_env(), "envs") and len(self.model.get_env().envs) > 0:
            env = self.model.get_env().envs[0]
            if hasattr(env, "env"):  # Handle potential wrappers
                env = env.env

            # Get portfolio metrics if available
            portfolio_value = getattr(env, "portfolio_value", 0)
            initial_balance = getattr(
                env, "initial_balance", 1
            )  # Avoid division by zero
            roi = (
                ((portfolio_value - initial_balance) / initial_balance) * 100
                if initial_balance > 0
                else 0
            )
        else:
            portfolio_value = 0
            roi = 0

        # Calculate episode statistics
        avg_reward = np.mean(self.episode_rewards[-10:]) if self.episode_rewards else 0
        avg_length = np.mean(self.episode_lengths[-10:]) if self.episode_lengths else 0
        fps = self.total_steps / elapsed_time if elapsed_time > 0 else 0

        # Get learning statistics
        learning_stats = {}
        if hasattr(self.model, "logger") and hasattr(
            self.model.logger, "name_to_value"
        ):
            learning_stats = self.model.logger.name_to_value

        # Calculate ETA
        total_steps = self.model._total_timesteps
        progress = (self.total_steps / total_steps) * 100 if total_steps > 0 else 0
        remaining_steps = max(0, total_steps - self.total_steps)
        eta_seconds = (remaining_steps / steps_per_sec) if steps_per_sec > 0 else 0
        eta_str = str(timedelta(seconds=int(eta_seconds)))

        # Print progress table
        print("\n" + "-" * 100)
        print(f"PROGRESSION: {progress:.1f}% | ETA: {eta_str} | FPS: {fps:.1f}")
        print("-" * 100)

        # Print status table
        print("\nSTATUT:")
        print(f"√âtape: {self.total_steps:,}/{total_steps:,} ({progress:.1f}%)")
        print(f"Temps √©coul√©: {str(timedelta(seconds=int(elapsed_time)))}")
        print(f"Temps restant estim√©: {eta_str}")
        print(f"Vitesse: {steps_per_sec:.1f} √©tapes/seconde")

        # Print performance metrics
        print("\nPERFORMANCE:")
        print(f"R√©compense moyenne (10 √©pisodes): {avg_reward:,.2f}")
        print(f"Longueur moyenne des √©pisodes: {avg_length:.1f} pas")
        print(f"Portefeuille: ${portfolio_value:,.2f} (ROI: {roi:+.2f}%)")

        # Print learning metrics if available
        if learning_stats:
            print("\nAPPRENTISSAGE:")
            for key, value in learning_stats.items():
                if (
                    "loss" in key.lower()
                    or "entropy" in key.lower()
                    or "value" in key.lower()
                ):
                    print(f"{key}: {value:.4f}")

        # Print system info
        print("\nSYST√àME:")
        try:
            print(f"Utilisation CPU: {psutil.cpu_percent()}%")
            try:
                process = psutil.Process()
                mem_info = process.memory_info()
                print(f"Utilisation m√©moire: {mem_info.rss / 1024 / 1024:.1f} MB")
            except Exception as e:
                print(f"Erreur lors de la lecture de l'utilisation m√©moire: {str(e)}")
        except ImportError:
            print("Utilisation CPU: Non disponible (psutil non install√©)")
        except Exception as e:
            print(f"Erreur lors de la lecture des informations syst√®me: {str(e)}")

        # Update last log time
        self.last_log_steps = self.total_steps
        self.last_time = current_time

    def _print_header(self) -> None:
        """Print the table header."""
        print("\n" + "=" * 100)
        print("SUIVI DE L'ENTRA√éNEMENT - ADAN TRADING BOT")
        print("=" * 100)
        print("D√©marrage √†:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        print("-" * 100)


class SB3GymCompatibilityWrapper(gym.Wrapper):
    """
    Wrapper pour assurer la compatibilit√© entre Gymnasium et Stable Baselines 3.
    G√®re sp√©cifiquement les espaces d'observation de type Dict pour les environnements de trading.
    """

    def __init__(self, env):
        super().__init__(env)

        # Enregistrer l'espace d'observation original
        self.original_obs_space = env.observation_space

        # S'assurer que l'espace d'action est correctement d√©fini
        if not isinstance(
            env.action_space,
            (spaces.Discrete, spaces.Box, spaces.MultiDiscrete, spaces.MultiBinary),
        ):
            raise ValueError(
                f"Type d'espace d'action non support√©: {type(env.action_space)}"
            )

        # Pour les environnements de trading avec espace d'observation de type Dict
        if isinstance(env.observation_space, spaces.Dict):
            # V√©rifier si nous avons les cl√©s attendues
            if (
                "observation" in env.observation_space.spaces
                and "portfolio_state" in env.observation_space.spaces
            ):
                # Enregistrer l'espace d'observation original
                self.observation_space = env.observation_space

                # Extraire les espaces pour le traitement
                obs_space = env.observation_space.spaces["observation"]
                portfolio_space = env.observation_space.spaces["portfolio_state"]

                # V√©rifier les dimensions
                expected_obs_shape = (
                    3,
                    20,
                    15,
                )  # Forme attendue: (timeframes, window_size, features)
                expected_portfolio_shape = (17,)  # Forme attendue pour le portefeuille

                # V√©rifier le type et la forme de l'observation
                if not (
                    isinstance(obs_space, spaces.Box) and len(obs_space.shape) == 3
                ):
                    raise ValueError(
                        f"Format d'observation non support√©. Attendu: Box 3D, obtenu: {type(obs_space)} avec forme {getattr(obs_space, 'shape', 'N/A')}"
                    )

                # V√©rifier le type et la forme du portefeuille
                if not (
                    isinstance(portfolio_space, spaces.Box)
                    and len(portfolio_space.shape) == 1
                ):
                    raise ValueError(
                        f"Format d'√©tat du portefeuille non support√©. Attendu: Box 1D, obtenu: {type(portfolio_space)} avec forme {getattr(portfolio_space, 'shape', 'N/A')}"
                    )

                # Avertissement si les formes ne correspondent pas exactement √† ce qui est attendu
                if obs_space.shape != expected_obs_shape:
                    logger.warning(
                        f"Forme d'observation non standard: {obs_space.shape}, attendu {expected_obs_shape}. "
                        "Le mod√®le peut n√©cessiter un ajustement."
                    )

                if portfolio_space.shape != expected_portfolio_shape:
                    logger.warning(
                        f"Forme de l'√©tat du portefeuille non standard: {portfolio_space.shape}, "
                        f"attendu {expected_portfolio_shape}. Le mod√®le peut n√©cessiter un ajustement."
                    )

                # Enregistrer les dimensions pour le traitement des observations
                self.obs_shape = obs_space.shape
                self.portfolio_dim = portfolio_space.shape[0]

                logger.info(
                    "SB3GymCompatibilityWrapper: Espace d'observation "
                    "configur√© avec succ√®s. Forme de l'observation: %s, "
                    "Dimension du portefeuille: %s",
                    obs_space.shape,
                    portfolio_space.shape,
                )
            else:
                raise ValueError(
                    "L'espace d'observation Dict doit contenir les cl√©s 'observation' et 'portfolio_state'"
                )
        else:
            # Pour les autres types d'espaces, utiliser l'espace d'observation tel quel
            self.observation_space = env.observation_space

    def reset(self, **kwargs):
        try:
            # Appeler reset sur l'environnement sous-jacent
            reset_result = self.env.reset(**kwargs)

            # Journalisation pour le d√©bogage
            logger.debug(
                f"SB3GymCompatibilityWrapper.reset - Type de sortie: {type(reset_result)}"
            )
            if isinstance(reset_result, tuple):
                logger.debug(
                    f"SB3GymCompatibilityWrapper.reset - Longueur du tuple: {len(reset_result)}"
                )

            # Extraire l'observation du r√©sultat de reset
            if isinstance(reset_result, tuple):
                if len(reset_result) == 2:
                    obs, info = reset_result  # Format (obs, info) de Gymnasium
                else:
                    # Si le tuple a une longueur diff√©rente, prendre le premier √©l√©ment comme observation
                    obs = reset_result[0] if len(reset_result) > 0 else {}
                    info = reset_result[1] if len(reset_result) > 1 else {}
            else:
                obs = reset_result
                info = {}

            # Journalisation suppl√©mentaire
            logger.debug(f"Type d'observation apr√®s extraction: {type(obs)}")
            if hasattr(obs, "shape"):
                logger.debug(f"Forme de l'observation: {obs.shape}")
            elif isinstance(obs, dict):
                logger.debug(f"Cl√©s de l'observation: {list(obs.keys())}")

            # Traiter l'observation pour la compatibilit√© avec SB3
            processed_obs = self._process_obs(obs)

            # Journalisation pour le d√©bogage
            logger.debug(
                f"SB3GymCompatibilityWrapper.reset - Type d'observation trait√©: {type(processed_obs)}"
            )
            if hasattr(processed_obs, "shape"):
                logger.debug(
                    f"SB3GymCompatibilityWrapper.reset - Forme de l'observation: {processed_obs.shape}"
                )

            # Pour la compatibilit√© avec SB3, retourner un tuple (observation, info)
            return processed_obs, info

        except Exception as e:
            logger.error(f"Erreur dans SB3GymCompatibilityWrapper.reset: {e}")
            logger.error(traceback.format_exc())
            # Retourner une observation par d√©faut en cas d'erreur
            default_shape = (3 * 20 * 15) + 17
            return np.zeros(default_shape, dtype=np.float32), {}

    def step(self, action):
        try:
            # Appeler step sur l'environnement sous-jacent
            step_result = self.env.step(action)

            # G√©rer les diff√©rents formats de retour
            if isinstance(step_result, tuple):
                if len(step_result) == 5:
                    # Format gymnasium : (obs, reward, terminated, truncated, info)
                    obs, reward, terminated, truncated, info = step_result
                    done = bool(terminated or truncated)
                elif len(step_result) == 4:
                    # Format gym ancien : (obs, reward, done, info)
                    obs, reward, done, info = step_result
                else:
                    raise ValueError(
                        f"Format de retour de step() non support√©: {step_result}"
                    )
            else:
                raise ValueError(
                    f"Le r√©sultat de step() doit √™tre un tuple, re√ßu: {type(step_result)}"
                )

            # Traiter l'observation pour la compatibilit√© avec SB3
            processed_obs = self._process_obs(obs)

            # Journalisation pour le d√©bogage
            logger.debug(
                f"Step - Type d'observation: {type(obs)}, "
                f"Type trait√©: {type(processed_obs)}, "
                f"Forme: {getattr(processed_obs, 'shape', 'N/A')}, "
                f"R√©compense: {reward:.4f}, "
                f"Termin√©: {done}"
            )

            # Retourner le format attendu par SB3 (4 valeurs)
            return processed_obs, float(reward), done, info

        except Exception as e:
            logger.error(f"Erreur dans SB3GymCompatibilityWrapper.step: {e}")
            logger.error(traceback.format_exc())
            # Retourner une observation par d√©faut en cas d'erreur
            default_shape = (3 * 20 * 15) + 17
            return np.zeros(default_shape, dtype=np.float32), 0.0, True, {}

    def _process_obs(self, obs):
        """
        Traite l'observation pour la compatibilit√© avec SB3.
        Convertit un dictionnaire d'observations en un tableau NumPy 1D.

        G√®re sp√©cifiquement les observations avec la forme (3, 20, 15) pour les donn√©es de march√©
        et (17,) pour l'√©tat du portefeuille.
        """
        # D√©finir les formes attendues
        expected_market_shape = (3, 20, 15)  # Forme attendue pour les donn√©es de march√©
        expected_portfolio_shape = (17,)  # Forme attendue pour l'√©tat du portefeuille

        # Fonction utilitaire pour cr√©er une observation par d√©faut
        def create_default_observation():
            market_obs = np.zeros(expected_market_shape, dtype=np.float32)
            portfolio_obs = np.zeros(expected_portfolio_shape, dtype=np.float32)
            return np.concatenate([market_obs.reshape(-1), portfolio_obs.reshape(-1)])

        try:
            # Si l'observation est d√©j√† un tableau numpy, la retourner telle quelle
            if isinstance(obs, np.ndarray):
                return obs.astype(np.float32)

            # Si c'est un dictionnaire avec les cl√©s attendues
            if (
                isinstance(obs, dict)
                and "observation" in obs
                and "portfolio_state" in obs
            ):
                # Valider le type et la forme des donn√©es d'observation
                if not isinstance(
                    obs["observation"], (np.ndarray, list, tuple)
                ) or not isinstance(obs["portfolio_state"], (np.ndarray, list, tuple)):
                    logger.warning(
                        "Type d'observation invalide. Utilisation d'une observation par d√©faut."
                    )
                    return create_default_observation()

                # Convertir les observations en tableaux NumPy
                try:
                    market_obs = np.array(obs["observation"], dtype=np.float32)
                    portfolio_obs = np.array(obs["portfolio_state"], dtype=np.float32)
                except Exception as e:
                    logger.error(f"Erreur lors de la conversion des observations: {e}")
                    return create_default_observation()

                # Valider et redimensionner les donn√©es de march√© si n√©cessaire
                if market_obs.shape != expected_market_shape:
                    logger.warning(
                        f"Forme des donn√©es de march√© non standard: {market_obs.shape}, "
                        f"attendu {expected_market_shape}. Redimensionnement en cours."
                    )
                    if len(market_obs.shape) == 3:
                        market_obs = market_obs[
                            :3, :20, :15
                        ]  # Tronquer aux dimensions attendues
                        if (
                            market_obs.shape[1] < 20
                        ):  # Si la dimension temporelle est trop petite
                            pad_width = [(0, 0), (0, 20 - market_obs.shape[1]), (0, 0)]
                            market_obs = np.pad(market_obs, pad_width, mode="constant")
                    else:
                        market_obs = np.zeros(expected_market_shape, dtype=np.float32)

                # Valider et redimensionner l'√©tat du portefeuille si n√©cessaire
                if portfolio_obs.shape != expected_portfolio_shape:
                    logger.warning(
                        f"Forme de l'√©tat du portefeuille non standard: {portfolio_obs.shape}, "
                        f"attendu {expected_portfolio_shape}. Redimensionnement en cours."
                    )
                    if len(portfolio_obs.shape) == 1 and portfolio_obs.size >= 17:
                        portfolio_obs = portfolio_obs[:17]  # Tronquer si trop grand
                    else:
                        portfolio_obs = np.zeros(
                            expected_portfolio_shape, dtype=np.float32
                        )

                # V√©rifier la forme finale avant de concat√©ner
                expected_market_size = np.prod(expected_market_shape)
                if market_obs.size != expected_market_size:
                    logger.warning(
                        f"Taille des donn√©es de march√© incorrecte: {market_obs.size}, "
                        f"attendu {expected_market_size}. Redimensionnement en cours."
                    )
                    market_obs = market_obs.reshape(-1)[:expected_market_size]
                    if market_obs.size < expected_market_size:
                        market_obs = np.pad(
                            market_obs,
                            (0, expected_market_size - market_obs.size),
                            mode="constant",
                        )
                    market_obs = market_obs.reshape(expected_market_shape)

                # Aplatir et concat√©ner les observations
                market_flat = market_obs.reshape(-1)
                portfolio_flat = portfolio_obs.reshape(-1)

                try:
                    processed_obs = np.concatenate([market_flat, portfolio_flat])

                    logger.debug(
                        f"Observation trait√©e - March√©: {market_obs.shape} -> {market_flat.shape}, "
                        f"Portefeuille: {portfolio_obs.shape} -> {portfolio_flat.shape}, "
                        f"Sortie: {processed_obs.shape}"
                    )
                except Exception as e:
                    logger.error(
                        f"Erreur lors de la concat√©nation des observations: {e}"
                    )
                    return create_default_observation()

                return processed_obs.astype(np.float32)

            # Si c'est un tuple (obs, info) de Gymnasium, extraire l'observation
            elif isinstance(obs, tuple) and len(obs) >= 2:
                logger.debug(
                    "Tuple d'observation d√©tect√©, extraction de l'√©l√©ment d'observation"
                )
                return self._process_obs(obs[0])  # Traiter uniquement l'observation

            # Si c'est une s√©quence (liste, tuple, etc.), essayer de la convertir en tableau
            elif isinstance(obs, (list, tuple)):
                logger.debug(
                    f"S√©quence d'observation d√©tect√©e, conversion en tableau: {type(obs)}"
                )
                try:
                    arr = np.array(obs, dtype=np.float32)
                    expected_size = np.prod(expected_market_shape) + len(
                        expected_portfolio_shape
                    )

                    if arr.size == expected_size:
                        return arr.reshape(-1).astype(np.float32)
                    elif arr.size > expected_size:
                        logger.warning(
                            f"Troncature de l'observation de taille {arr.size} √† {expected_size}"
                        )
                        return arr.flat[:expected_size].astype(np.float32)
                    else:
                        logger.warning(
                            f"Remplissage de l'observation de taille {arr.size} √† {expected_size}"
                        )
                        return np.pad(
                            arr.reshape(-1),
                            (0, expected_size - arr.size),
                            mode="constant",
                        ).astype(np.float32)
                except Exception as e:
                    logger.error(f"Erreur lors de la conversion de la s√©quence: {e}")
                    raise

            # Pour les autres types, essayer une conversion directe
            logger.warning(
                f"Type d'observation non standard, tentative de conversion: {type(obs)}"
            )
            try:
                arr = np.array(obs, dtype=np.float32)
                expected_size = np.prod(expected_market_shape) + len(
                    expected_portfolio_shape
                )

                if arr.size == expected_size:
                    return arr.reshape(-1).astype(np.float32)
                elif arr.size > expected_size:
                    logger.warning(
                        f"Troncature de l'observation de taille {arr.size} √† {expected_size}"
                    )
                    return arr.flat[:expected_size].astype(np.float32)
                else:
                    logger.warning(
                        f"Remplissage de l'observation de taille {arr.size} √† {expected_size}"
                    )
                    return np.pad(
                        arr.reshape(-1), (0, expected_size - arr.size), mode="constant"
                    ).astype(np.float32)
            except Exception as e:
                logger.error(f"√âchec de la conversion de l'observation: {e}")
                return create_default_observation()

        except Exception as e:
            logger.error(f"Erreur critique lors du traitement de l'observation: {e}")
            logger.error(f"Type d'observation: {type(obs).__name__}")

            # Journalisation d√©taill√©e pour le d√©bogage
            if hasattr(obs, "shape"):
                logger.error(f"Forme de l'observation: {obs.shape}")
            elif hasattr(obs, "__len__"):
                logger.error(f"Longueur de l'observation: {len(obs)}")

            if isinstance(obs, dict):
                logger.error("Cl√©s de l'observation:")
                for k, v in obs.items():
                    type_info = type(v).__name__
                    shape_info = f", shape={v.shape}" if hasattr(v, "shape") else ""
                    len_info = f", len={len(v)}" if hasattr(v, "__len__") else ""
                    logger.error(f"  {k}: type={type_info}{shape_info}{len_info}")

            logger.error("Traceback complet de l'erreur:", exc_info=True)

        # En cas d'erreur, retourner un tableau de z√©ros de la bonne dimension
        default_market = np.zeros(expected_market_shape, dtype=np.float32)
        default_portfolio = np.zeros(expected_portfolio_shape, dtype=np.float32)
        default_obs = np.concatenate([default_market.reshape(-1), default_portfolio])

        logger.warning(
            f"Retour d'une observation par d√©faut de forme {default_obs.shape} "
            f"(march√©: {default_market.shape}, portefeuille: {default_portfolio.shape})"
        )

        return default_obs


def make_env(
    rank: int = 0,
    seed: Optional[int] = None,
    config: Dict = None,
    worker_config: Dict = None,
    dbe_registry: Dict = None,
) -> gym.Env:
    """
    Cr√©e et configure un environnement pour un worker donn√©.

    Args:
        rank: Identifiant unique du worker
        seed: Graine pour la reproductibilit√©
        config: Configuration de l'environnement
        worker_config: Configuration sp√©cifique au worker

    Returns:
        Un environnement Gym valide
    """
    # Configuration par d√©faut si config n'est pas fourni
    if config is None:
        config = {}

    # Configuration du worker si non fournie
    if worker_config is None:
        # R√©cup√©rer la liste des actifs depuis la configuration
        assets = [a.lower() for a in config.get("data", {}).get("assets", ["btcusdt"])]
        timeframes = [
            str(tf).lower()
            for tf in config.get("data", {}).get("timeframes", ["5m", "1h", "4h"])
        ]
        data_split = config.get("data", {}).get("data_split", "val").lower()

        worker_config = {
            "rank": rank,
            "worker_id": clean_worker_id(
                rank
            ),  # ID nettoy√© pour √©viter les erreurs JSONL
            "num_workers": config.get("num_workers", 1),
            "assets": assets,
            "timeframes": timeframes,
            "data_split": data_split,
            "data_loader": {
                "batch_size": config.get("batch_size", 32),
                "shuffle": True,
                "num_workers": 0,  # D√©sactiver le multithreading pour √©viter les probl√®mes
            },
        }
        logger.info(
            f"[WORKER-{rank}] Configuration worker cr√©√©e: assets={assets}, timeframes={timeframes}, data_split={data_split}"
        )

    # Extraire les param√®tres n√©cessaires de la configuration
    data_config = config.get("data", {})
    env_config = config.get("environment", {})

    # R√©cup√©rer les param√®tres du worker
    assets = [a for a in worker_config.get("assets", [])]
    timeframes = [str(tf).lower() for tf in worker_config.get("timeframes", [])]

    # R√©cup√©rer le data_split de la configuration du worker, avec une valeur par d√©faut
    data_split = worker_config.get("data_split", "val").lower()

    # Utiliser des chemins absolus avec fallbacks intelligents
    possible_paths = [
        Path("/home/morningstar/Documents/trading/data/processed/indicators")
        / data_split,
        Path("data/processed/indicators") / data_split,
        Path(__file__).parent.parent.parent
        / "data"
        / "processed"
        / "indicators"
        / data_split,
        Path(__file__).parent.parent / "data" / "processed" / "indicators" / data_split,
    ]

    data_dir = None
    for path in possible_paths:
        if path.exists():
            data_dir = path
            break

    # Si aucun chemin n'existe, utiliser le premier comme d√©faut
    if data_dir is None:
        data_dir = possible_paths[0]

    logger.info(f"Data split utilis√©: {data_split}")
    logger.info(f"Dossier de donn√©es final: {data_dir}")

    # Cr√©er le r√©pertoire s'il n'existe pas (pour les tests)
    if not data_dir.exists():
        logger.warning(
            f"R√©pertoire de donn√©es {data_dir} non trouv√©, cr√©ation en cours..."
        )
        data_dir.mkdir(parents=True, exist_ok=True)

        # Cr√©er des donn√©es factices pour les tests si aucune donn√©e n'existe
        if pd is None or np is None:
            logger.error(
                "Pandas ou NumPy non disponible pour cr√©er des donn√©es factices"
            )
            raise ValueError(
                f"Le r√©pertoire de donn√©es {data_dir} n'existe pas et impossible de cr√©er des donn√©es factices"
            )

        for asset in assets:
            clean_asset = asset.replace("/", "").replace("-", "")
            asset_dir = data_dir / clean_asset.upper()
            asset_dir.mkdir(exist_ok=True)

            for tf in timeframes:
                file_path = asset_dir / f"{tf}.parquet"
                if not file_path.exists():
                    # Cr√©er des donn√©es factices pour le test
                    dates = pd.date_range("2024-01-01", periods=1000, freq="5min")
                    fake_data = pd.DataFrame(
                        {
                            "timestamp": dates,
                            "open": np.random.uniform(0.5, 1.0, 1000),
                            "high": np.random.uniform(0.5, 1.0, 1000),
                            "low": np.random.uniform(0.5, 1.0, 1000),
                            "close": np.random.uniform(0.5, 1.0, 1000),
                            "volume": np.random.uniform(1000, 10000, 1000),
                        }
                    )

                    # Ajouter des indicateurs techniques factices
                    for i in range(10):  # 10 indicateurs factices
                        fake_data[f"indicator_{i}"] = np.random.uniform(-1, 1, 1000)

                    fake_data.to_parquet(file_path, index=False)
                    logger.info(f"Donn√©es factices cr√©√©es: {file_path}")

    logger.info(f"Chargement des donn√©es depuis : {data_dir}")
    logger.info(f"Actifs: {assets}")
    logger.info(f"Timeframes: {timeframes}")
    logger.info(f"Split de donn√©es: {data_split}")

    # Dictionnaire pour stocker les donn√©es charg√©es
    data = {}
    data_found = False

    # Charger les donn√©es pour chaque actif et chaque timeframe
    for asset in assets:
        # Nettoyer le nom de l'actif (supprimer / et -) et forcer en minuscules
        clean_asset = asset.replace("/", "").replace("-", "")
        data[clean_asset] = {}
        asset_data_found = False

        for tf in timeframes:
            # Construire le chemin du fichier dans le format: {split}/{asset}/{timeframe}.parquet
            file_path = data_dir / clean_asset / f"{tf}.parquet"

            # V√©rifier si le fichier existe
            if not file_path.exists():
                logger.warning(f"Fichier non trouv√©: {file_path}")
                continue

            try:
                # Charger les donn√©es Parquet
                df = pd.read_parquet(file_path)

                # V√©rifier que le DataFrame n'est pas vide
                if df.empty:
                    logger.warning(f"Le fichier {file_path} est vide.")
                    continue

                # Stocker les donn√©es dans la structure attendue
                data[clean_asset.upper()][tf] = df
                logger.info(
                    f"Donn√©es charg√©es: {clean_asset.upper()}/{tf} - {len(df)} lignes"
                )
                data_found = True
                asset_data_found = True

            except Exception as e:
                logger.error(f"Erreur lors du chargement de {file_path}: {str(e)}")

        if not asset_data_found:
            logger.warning(
                f"Aucune donn√©e valide trouv√©e pour l'actif {clean_asset.upper()}"
            )
            del data[clean_asset.upper()]

    if not data:
        raise ValueError(
            "Aucune donn√©e valide n'a pu √™tre charg√©e. "
            f"V√©rifiez les chemins dans la configuration et assurez-vous que les fichiers existent dans {data_dir}."
        )

    # D√©finir la taille de la fen√™tre et la configuration des caract√©ristiques
    window_size = config.get("environment", {}).get("window_size", 50)
    features_config = config.get("environment", {}).get("features_config", {})

    logger.info(f"Taille de la fen√™tre: {window_size}")
    logger.info(f"Configuration des caract√©ristiques: {features_config}")

    # SOLUTION IMMORTALIT√â ADAN: R√©cup√©rer ou cr√©er le DBE pour ce worker
    worker_id = worker_config.get("worker_id", f"w{rank}")
    existing_dbe = None

    if dbe_registry is not None:
        existing_dbe = dbe_registry.get(worker_id)
        if existing_dbe is not None:
            logger.critical(
                f"üëë R√âUTILISATION DBE IMMORTEL r√©ussie pour Worker {worker_id}, DBE_ID={id(existing_dbe)}"
            )
        else:
            logger.critical(f"üÜï CR√âATION PREMIER DBE IMMORTEL pour Worker {worker_id}")

    # Cr√©er l'environnement avec les donn√©es charg√©es
    env = MultiAssetChunkedEnv(
        data=data,
        timeframes=timeframes,
        window_size=window_size,
        features_config=features_config,
        max_steps=env_config.get("max_steps", 1000),
        initial_balance=env_config.get("initial_balance", 10000.0),
        commission=env_config.get("commission", 0.001),
        reward_scaling=env_config.get("reward_scaling", 1.0),
        render_mode=None,
        enable_logging=config.get("enable_logging", True),
        log_dir=config.get("log_dir", "logs"),
        worker_config=worker_config,
        config=config,
        external_dbe=existing_dbe,  # Passer le DBE existant s'il y en a un
    )

    # SOLUTION IMMORTALIT√â ADAN: Enregistrer le DBE nouvellement cr√©√© dans le registre
    if dbe_registry is not None and existing_dbe is None:
        if hasattr(env, "dbe") and env.dbe is not None:
            dbe_registry[worker_id] = env.dbe
            logger.critical(
                f"üíæ DBE IMMORTEL SAUVEGARD√â dans registre pour Worker {worker_id}, DBE_ID={id(env.dbe)}"
            )
        else:
            logger.error(
                f"‚ùå √âCHEC sauvegarde DBE pour Worker {worker_id} - DBE non trouv√© dans l'environnement"
            )

    # Appliquer le wrapper pour la compatibilit√© et √©viter les logs dupliqu√©s
    env = GymnasiumToGymWrapper(env, rank=rank)

    # Configurer la graine pour la reproductibilit√©
    if seed is not None:
        try:
            # Essayer d'utiliser np.random pour la graine
            import numpy as np

            np.random.seed(seed)

            # Configurer la graine de l'environnement si possible
            base = getattr(env, "unwrapped", env)
            if hasattr(base, "seed"):
                base.seed(seed)

            # Configurer la graine de l'espace d'action
            if hasattr(env, "action_space") and hasattr(env.action_space, "seed"):
                env.action_space.seed(seed)

            # Configurer la graine de l'espace d'observation
            if hasattr(env, "observation_space") and hasattr(
                env.observation_space, "seed"
            ):
                env.observation_space.seed(seed)

        except Exception as e:
            logger.warning(
                f"Impossible de configurer la graine pour l'environnement {rank}: {str(e)}"
            )

    logger.info(f"Environnement {rank} cr√©√© avec succ√®s")
    return env


def main(
    config_path: str = "bot/config/config.yaml",
    timeout: Optional[int] = None,
    checkpoint_dir: str = "/mnt/new_data/trading_bot_checkpoints",
    shared_model_path: Optional[str] = None,
    resume: bool = False,
    num_envs: int = 4,
    use_subproc: bool = False,
) -> bool:
    """
    Fonction principale pour l'entra√Ænement parall√®le des agents ADAN.

    Args:
        config_path: Chemin vers le fichier de configuration YAML
        timeout: D√©lai maximum d'entra√Ænement en secondes
        checkpoint_dir: R√©pertoire pour enregistrer les points de contr√¥le
        shared_model_path: Chemin vers un mod√®le partag√© pour l'entra√Ænement distribu√©
        resume: Reprendre l'entra√Ænement √† partir du dernier point de contr√¥le
        num_envs: Nombre d'environnements parall√®les √† ex√©cuter
        use_subproc: Si True, utilise SubprocVecEnv au lieu de DummyVecEnv

    Returns:
        bool: True si l'entra√Ænement s'est termin√© avec succ√®s, False sinon
    """
    try:
        # Supprimer tous les logs de debug pour un affichage plus propre
        logging.getLogger().setLevel(logging.ERROR)

        # Validate environment (Python version, deps, etc.)
        try:
            validate_environment()
        except Exception as e:
            print(f"Environment validation failed: {e}")
            raise

        # Charger la configuration
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)

        # R√©soudre les variables de configuration (version simplifi√©e)
        config = resolve_config_variables(config)

        print("üöÄ ADAN Training Bot - Configuration charg√©e")
        print(f"‚è±Ô∏è  Timeout configur√©: {timeout} secondes")
        # Activer ou non la barre de progression pendant l'entra√Ænement (config training.progress_bar)
        progress_bar = bool(config.get("training", {}).get("progress_bar", False))

        # R√©cup√©rer la liste des actifs et timeframes depuis la configuration
        data_config = config.get("data", {})
        file_structure = data_config.get("file_structure", {})
        assets = data_config.get("assets", ["BTCUSDT"])
        timeframes = file_structure.get("timeframes", ["5m", "1h", "4h"])
        seed = config.get("seed", 42)

        # Configuration commune pour tous les workers
        base_worker_config = {
            "num_workers": num_envs,
            "assets": assets,  # Liste des actifs
            "timeframes": timeframes,  # Liste des timeframes
            "chunk_sizes": {
                tf: 1000 for tf in timeframes
            },  # Chunk size augment√©e pour le warm-up
            "data_loader": {
                "batch_size": config.get("batch_size", 32),
                "shuffle": True,
                "num_workers": 0,  # Important: laisser √† 0 pour √©viter les probl√®mes de fork
            },
            "use_subproc": use_subproc,  # Passer l'info au worker
            "enable_worker_id_logging": True,  # Activer les IDs workers pour r√©duire r√©p√©titions
        }

        # Choisir la classe d'environnement vectoris√©
        VecEnvClass = SubprocVecEnv if use_subproc else DummyVecEnv

        # SOLUTION IMMORTALIT√â ADAN: Initialiser le registre global des DBE
        logger.critical(f"üèõÔ∏è INITIALISATION REGISTRE GLOBAL DBE pour {num_envs} workers")

        # Configurer les arguments pour SubprocVecEnv
        vec_env_kwargs = {}
        if use_subproc:
            # Activer les logs de debug du multiprocessing pour voir les erreurs des workers
            try:
                import multiprocessing.util as mp_util
                mp_util.log_to_stderr(logging.INFO)
            except Exception:
                pass

            # Utiliser 'spawn' pour √©viter l'h√©ritage d'√©tat probl√©matique avec certaines libs
            vec_env_kwargs.update({"start_method": "spawn"})
            logger.info(
                f"Utilisation de SubprocVecEnv avec {num_envs} processus parall√®les (start_method='spawn')"
            )
        else:
            logger.info(f"Utilisation de DummyVecEnv (sans parall√©lisme r√©el)")

        # Cr√©er les fonctions de cr√©ation d'environnement
        def make_env_fn(rank: int, seed_val: int) -> Callable[[], gym.Env]:
            """Cr√©e une fonction d'initialisation d'environnement pour un rang donn√©."""
            def _init() -> gym.Env:
                # Worker-specific config
                worker_config = base_worker_config.copy()
                worker_config.update({
                    "rank": rank,
                    "seed": seed_val,
                    "worker_id": f"worker_{rank}"
                })

                # Build required constructor args for MultiAssetChunkedEnv
                try:
                    # Assets and placeholder data dict
                    assets = config.get("data", {}).get("assets", ["BTCUSDT"]) or ["BTCUSDT"]
                    data_placeholder = {str(asset): {} for asset in assets}

                    # Timeframes
                    tfs = (
                        config.get("environment", {})
                              .get("observation", {})
                              .get("timeframes", config.get("data", {}).get("timeframes", ["5m", "1h", "4h"]))
                    )

                    # Window size
                    window_size = config.get("environment", {}).get("window_size", 20)

                    # Features config per timeframe (use observation features lists)
                    obs_features = (
                        config.get("environment", {})
                              .get("observation", {})
                              .get("features", {})
                    )
                    base_feats = obs_features.get("base", []) or ["OPEN", "HIGH", "LOW", "CLOSE", "VOLUME"]
                    tf_indicators = obs_features.get("indicators", {})
                    features_config = {}
                    for tf in tfs:
                        tf_inds = tf_indicators.get(tf, [])
                        # Combine base + tf indicators (keep as-is; only length is used early)
                        features_config[str(tf)] = list(base_feats) + list(tf_inds)

                    # Environment scalars
                    env_cfg = config.get("environment", {})
                    max_steps = env_cfg.get("max_steps", 100000)
                    initial_balance = env_cfg.get("initial_balance", 10000.0)
                    commission = env_cfg.get("commission", 0.001)
                    reward_scaling = env_cfg.get("reward_scaling", 1.0)

                    return MultiAssetChunkedEnv(
                        data=data_placeholder,
                        timeframes=tfs,
                        window_size=int(window_size),
                        features_config=features_config,
                        max_steps=int(max_steps),
                        initial_balance=float(initial_balance),
                        commission=float(commission),
                        reward_scaling=float(reward_scaling),
                        worker_config=worker_config,
                        config=config,
                        total_workers=num_envs,
                    )
                except Exception as e:
                    logger.error(f"[WORKER-{rank}] Failed to build env args: {e}", exc_info=True)
                    raise
            return _init

        # Cr√©er l'environnement vectoris√©
        try:
            env = SubprocVecEnv(
                [make_env_fn(i, seed + i * 1000 if seed is not None else None) for i in range(num_envs)],
                start_method='spawn'
            )
            logger.info(f"Environnement vectoris√© 'SubprocVecEnv' cr√©√© avec succ√®s.")
        except Exception as e:
            logger.error(f"Erreur lors de la cr√©ation de l'environnement vectoris√©: {e}", exc_info=True)
            raise

        # Ajouter la normalisation des observations si n√©cessaire
        if config.get("normalize_observations", True):
            env = VecNormalize(env, norm_obs=True, norm_reward=True)
        # Configuration optimis√©e de l'agent PPO avec MultiInputPolicy pour g√©rer les espaces d'observation de type dictionnaire

        # V√©rifier l'espace d'observation
        logger.info(f"Espace d'observation de l'environnement: {env.observation_space}")

        # Configuration du r√©seau de neurones pour la politique r√©currente
        policy_kwargs = {
            # La dimension des features est d√©finie dans la politique (128 par d√©faut)
            # et pass√©e √† l'extracteur TemporalFusionExtractor.
            "lstm_hidden_size": 256, # Taille de l'√©tat cach√© du LSTM
            "n_lstm_layers": 2, # Nombre de couches LSTM
            "net_arch": dict(pi=[128, 64], vf=[128, 64]), # Couches apr√®s le LSTM
            "activation_fn": th.nn.ReLU,
        }

        # Cr√©er ou charger le mod√®le PPO
        model = None
        latest_checkpoint = None

        # Afficher des informations sur le parall√©lisme
        logger.info("\n" + "=" * 80)
        logger.info(f"Configuration de l'entra√Ænement:")
        logger.info(f"- Nombre d'environnements parall√®les: {num_envs}")
        logger.info(
            f"- Type d'environnement: {'SubprocVecEnv' if use_subproc else 'DummyVecEnv'}"
        )
        logger.info(f"- Seed de base: {config.get('seed', 42)}")
        logger.info(f"- Device: {'auto'}")
        logger.info("=" * 80 + "\n")

        # V√©rifier si on doit reprendre depuis un checkpoint
        # Cr√©er le r√©pertoire de checkpoints si n√©cessaire
        os.makedirs(checkpoint_dir, exist_ok=True)

        # Initialiser le CheckpointManager
        checkpoint_manager = CheckpointManager(
            checkpoint_dir=checkpoint_dir,
            max_checkpoints=5,  # Garder les 5 derniers checkpoints
            checkpoint_interval=10000,  # Sauvegarder tous les 10 000 steps
            logger=logger,
        )

        # V√©rifier si on doit reprendre depuis un checkpoint
        if resume and checkpoint_manager.list_checkpoints():
            latest_checkpoint = checkpoint_manager.list_checkpoints()[-1]
            logger.info(
                f"Reprise de l'entra√Ænement depuis le checkpoint: {latest_checkpoint}"
            )

            # Cr√©er un mod√®le minimal pour le chargement (RecurrentPPO)
            if RecurrentPPO is None:
                raise ImportError("sb3-contrib (RecurrentPPO) est requis. Installez-le: pip install sb3-contrib")
            model = RecurrentPPO(
                policy=CustomRecurrentPolicy,
                env=env,
                policy_kwargs=policy_kwargs if 'policy_kwargs' in locals() else None,
                verbose=1,
                seed=config.get("seed", 42),
                device="auto",
            )
            # Configure SB3 logger for resume case
            try:
                sb3_log_dir = Path("reports/tensorboard_logs")
                sb3_log_dir.mkdir(parents=True, exist_ok=True)
                new_logger = sb3_logger_configure(
                    str(sb3_log_dir), ["stdout", "csv", "tensorboard"]
                )
                model.set_logger(new_logger)
                logger.info(f"SB3 logger configured at {sb3_log_dir}")
            except Exception as e:
                logger.warning(f"Failed to configure SB3 logger: {e}")

            # Charger le checkpoint
            model, _, metadata = checkpoint_manager.load_checkpoint(
                checkpoint_path=latest_checkpoint, model=model, map_location="auto"
            )

            if metadata:
                logger.info(
                    "Checkpoint charg√© - √âpisode: %d, Steps: %d",
                    metadata.episode,
                    metadata.total_steps,
                )
                start_timesteps = metadata.total_steps
            else:
                logger.warning(
                    "M√©tadonn√©es du checkpoint non trouv√©es, d√©marrage √† z√©ro"
                )
                start_timesteps = 0

        # Si pas de reprise ou √©chec de chargement, cr√©er un nouveau mod√®le
        if model is None:
            logger.info("Cr√©ation d'un nouveau mod√®le")
            if RecurrentPPO is None:
                raise ImportError("sb3-contrib (RecurrentPPO) est requis. Installez-le: pip install sb3-contrib")
            model = RecurrentPPO(
                policy=CustomRecurrentPolicy,
                env=env,
                learning_rate=3e-4,
                n_steps=2048,
                batch_size=64,
                n_epochs=10,
                gamma=0.99,
                gae_lambda=0.95,
                clip_range=0.2,
                ent_coef=0.0,
                vf_coef=0.5,
                max_grad_norm=0.5,
                policy_kwargs=policy_kwargs if 'policy_kwargs' in locals() else None,
                verbose=1,
                seed=config.get("seed", 42),
                device="auto",
            )
            # Configure SB3 logger for fresh model
            try:
                sb3_log_dir = Path("reports/tensorboard_logs")
                sb3_log_dir.mkdir(parents=True, exist_ok=True)
                new_logger = sb3_logger_configure(
                    str(sb3_log_dir), ["stdout", "csv", "tensorboard"]
                )
                model.set_logger(new_logger)
                logger.info(f"SB3 logger configured at {sb3_log_dir}")
            except Exception as e:
                logger.warning(f"Failed to configure SB3 logger: {e}")
            start_timesteps = 0

        # V√©rifier si on doit reprendre depuis un checkpoint
        if resume:
            try:
                checkpoints = checkpoint_manager.list_checkpoints()
                if not checkpoints:
                    logger.info(
                        "[TRAINING] Aucun checkpoint trouv√©, d√©marrage d'un nouvel entra√Ænement"
                    )
                    start_timesteps = 0
                else:
                    latest_checkpoint = checkpoints[-1]
                    logger.info(
                        f"[TRAINING] Tentative de reprise depuis le checkpoint: {latest_checkpoint}"
                    )

                    # V√©rifier si le checkpoint existe toujours
                    if not os.path.exists(latest_checkpoint):
                        logger.warning(
                            f"[TRAINING] Le checkpoint {latest_checkpoint} n'existe plus"
                        )
                        start_timesteps = 0
                    else:
                        # Cr√©er un mod√®le minimal pour le chargement (RecurrentPPO)
                        if RecurrentPPO is None:
                            raise ImportError("sb3-contrib (RecurrentPPO) est requis. Installez-le: pip install sb3-contrib")
                        model = RecurrentPPO(
                            policy=CustomRecurrentPolicy,
                            env=env,
                            policy_kwargs=policy_kwargs if 'policy_kwargs' in locals() else None,
                            verbose=1,
                            seed=config.get("seed", 42),
                            device="auto",
                        )

                        try:
                            # Charger le checkpoint
                            model, optimizer, metadata = (
                                checkpoint_manager.load_checkpoint(
                                    checkpoint_path=latest_checkpoint,
                                    model=model,
                                    map_location="auto",
                                )
                            )

                            if metadata is not None:
                                logger.info(
                                    "[TRAINING] Checkpoint charg√© - √âpisode: %d, Steps: %d",
                                    metadata.episode,
                                    metadata.total_steps,
                                )
                                start_timesteps = metadata.total_steps

                                # V√©rifier la coh√©rence des m√©tadonn√©es
                                if not hasattr(
                                    metadata, "total_steps"
                                ) or not isinstance(metadata.total_steps, int):
                                    logger.warning(
                                        "[TRAINING] M√©tadonn√©es de checkpoint invalides, r√©initialisation du compteur d'√©tapes"
                                    )
                                    start_timesteps = 0
                            else:
                                logger.warning(
                                    "[TRAINING] Aucune m√©tadonn√©e trouv√©e dans le checkpoint"
                                )
                                start_timesteps = 0

                        except Exception as e:
                            logger.error(
                                f"[TRAINING] Erreur lors du chargement du checkpoint: {str(e)}",
                                exc_info=True,
                            )
                            logger.warning(
                                "[TRAINING] D√©marrage d'un nouvel entra√Ænement"
                            )
                            start_timesteps = 0

            except Exception as e:
                logger.error(
                    f"[TRAINING] Erreur lors de la v√©rification des checkpoints: {str(e)}",
                    exc_info=True,
                )
                logger.warning("[TRAINING] D√©marrage d'un nouvel entra√Ænement")
                start_timesteps = 0
        else:
            logger.info("[TRAINING] D√©marrage d'un nouvel entra√Ænement (sans reprise)")
            start_timesteps = 0

        class CustomCheckpointCallback(BaseCallback):
            """
            Callback personnalis√© pour la sauvegarde des checkpoints.
            """

            def __init__(self, checkpoint_manager: CheckpointManager, verbose: int = 0):
                """
                Initialise le callback de sauvegarde.

                Args:
                    checkpoint_manager: Gestionnaire de checkpoints
                    verbose: Niveau de verbosit√©
                """
                super().__init__(verbose)
                self.checkpoint_manager = checkpoint_manager
                self.last_save = 0
                self.last_checkpoint = None
                self.episode_rewards = []
                self._last_checkpoint_step = (
                    0  # Pour suivre la derni√®re √©tape de sauvegarde
                )

            def _on_step(self) -> bool:
                """
                Appel√© √† chaque √©tape d'entra√Ænement pour g√©rer la sauvegarde des checkpoints.

                Returns:
                    bool: True pour continuer l'entra√Ænement, False pour l'arr√™ter
                """
                # V√©rifier si c'est le moment de sauvegarder un checkpoint
                if (
                    self.num_timesteps - self._last_checkpoint_step
                ) >= self.checkpoint_manager.checkpoint_interval:
                    self._save_checkpoint()
                    self._last_checkpoint_step = self.num_timesteps
                return True

            def _save_checkpoint(self):
                """
                Sauvegarde un checkpoint du mod√®le.
                """
                try:
                    # R√©cup√©rer les m√©triques actuelles
                    metrics = {}
                    if (
                        hasattr(self.model, "ep_info_buffer")
                        and len(self.model.ep_info_buffer) > 0
                    ):
                        # Calculer les statistiques de r√©compense sur les derniers √©pisodes
                        rewards = [
                            ep_info["r"]
                            for ep_info in self.model.ep_info_buffer
                            if "r" in ep_info
                        ]
                        if rewards:
                            metrics["mean_reward"] = float(np.mean(rewards))
                            metrics["min_reward"] = float(np.min(rewards))
                            metrics["max_reward"] = float(np.max(rewards))
                            metrics["num_episodes"] = len(rewards)

                    # R√©cup√©rer le num√©ro d'√©pisode actuel si disponible
                    episode = getattr(self.model, "_episode_num", 0)

                    # Sauvegarder le checkpoint
                    checkpoint_path = self.checkpoint_manager.save_checkpoint(
                        model=self.model,
                        optimizer=self.model.policy.optimizer
                        if hasattr(self.model.policy, "optimizer")
                        else None,
                        episode=episode,
                        total_steps=self.num_timesteps,
                        metrics=metrics,
                        custom_data={
                            "config": config,
                            "policy_kwargs": policy_kwargs,
                            "env_config": config.get("env", {}),
                        },
                        is_final=False,
                    )

                    if checkpoint_path:
                        self.last_checkpoint = checkpoint_path
                        self.last_save = self.num_timesteps
                        logger.info(
                            "Checkpoint sauvegard√© (√©tape %d, √©pisode %d, r√©compense moyenne: %s)",
                            self.num_timesteps,
                            episode,
                            metrics.get("mean_reward", "N/A"),
                        )

                        # Nettoyer les anciens checkpoints si n√©cessaire
                        self.checkpoint_manager._cleanup_old_checkpoints()

                except Exception as e:
                    logger.error(
                        "Erreur lors de la sauvegarde du checkpoint: %s",
                        str(e),
                        exc_info=True,
                    )

            def _on_training_end(self) -> None:
                """
                Appel√© √† la fin de l'entra√Ænement pour sauvegarder un checkpoint final.
                """
                try:
                    # Sauvegarder un checkpoint final
                    metrics = {}
                    if (
                        hasattr(self.model, "ep_info_buffer")
                        and len(self.model.ep_info_buffer) > 0
                    ):
                        rewards = [
                            ep_info["r"]
                            for ep_info in self.model.ep_info_buffer
                            if "r" in ep_info
                        ]
                        if rewards:
                            metrics["final_mean_reward"] = float(np.mean(rewards))

                    episode = getattr(self.model, "_episode_num", 0)

                    checkpoint_path = self.checkpoint_manager.save_checkpoint(
                        model=self.model,
                        optimizer=self.model.policy.optimizer
                        if hasattr(self.model.policy, "optimizer")
                        else None,
                        episode=episode,
                        total_steps=self.num_timesteps,
                        metrics=metrics,
                        custom_data={
                            "config": config,
                            "policy_kwargs": policy_kwargs,
                            "env_config": config.get("env", {}),
                            "training_completed": True,
                        },
                        is_final=True,
                    )

                    if checkpoint_path:
                        logger.info(
                            "[TRAINING] Checkpoint final sauvegard√©: %s (√©tape %d, √©pisode %d)",
                            checkpoint_path,
                            self.num_timesteps,
                            episode,
                        )

                except Exception as e:
                    logger.error(
                        "[TRAINING] Erreur lors de la sauvegarde du checkpoint final: %s",
                        str(e),
                        exc_info=True,
                    )

        # Cr√©er le callback de sauvegarde des checkpoints
        checkpoint_callback = CustomCheckpointCallback(
            checkpoint_manager=checkpoint_manager, verbose=1
        )

        # Initialiser la liste des callbacks avec le checkpoint
        callbacks = [checkpoint_callback]

        # Ajouter le callback hi√©rarchique pour l'affichage structur√© (maintenant compatible Subproc)
        total_timesteps = config.get("training", {}).get("total_timesteps", 1000000)
        initial_capital = config.get("environment", {}).get("initial_balance", 10000.0)
        hierarchical_callback = HierarchicalTrainingCallback(
            verbose=1,
            display_freq=1000,
            total_timesteps=total_timesteps,
            initial_capital=initial_capital,
        )
        callbacks.append(hierarchical_callback)
        logger.info(
            "[TRAINING] Affichage hi√©rarchique activ√© avec m√©triques d√©taill√©es"
        )

        # En mode non-Subproc, ajouter les autres callbacks intrusifs
        if not use_subproc:
            # Ajouter le callback pour la visualisation de l'attention si activ√©
            if config.get('model', {}).get('diagnostics', {}).get('save_attention_maps', False):
                attention_cb = AttentionVisualizerCallback(
                    viz_cfg=config['model']['diagnostics'],
                    verbose=1
                )
                callbacks.append(attention_cb)

            # Ajouter le callback de progression personnalis√© si activ√© dans la config
            use_custom_progress = config.get("training", {}).get(
                "use_custom_progress", False
            )
            if use_custom_progress:
                progress_callback = CustomTrainingInfoCallback(check_freq=1000, verbose=1)
                callbacks.append(progress_callback)
                logger.info(
                    "[TRAINING] Barre de progression personnalis√©e activ√©e pour le suivi de l'entra√Ænement"
                )


            metrics_log_dir = os.path.join(
                config.get("paths", {}).get("logs_dir", "logs"),
                "training_metrics",
            )
            metrics_log_interval = (
                config.get("monitoring", {}).get("metrics_log_interval")
                or config.get("training", {}).get("metrics_log_interval")
                or 500
            )
            worker_metrics_callback = WorkerMetricsLogger(
                log_dir=metrics_log_dir,
                initial_balance=initial_capital,
                log_interval=metrics_log_interval,
                tensorboard_prefix="workers",
            )
            callbacks.append(worker_metrics_callback)
            logger.info(
                "[TRAINING] Journalisation des m√©triques par worker activ√©e (%s)",
                metrics_log_dir,
            )

        # Cr√©er un CallbackList pour g√©rer plusieurs callbacks
        callback = CallbackList(callbacks)

        logger.info(
            f"[TRAINING] {len(callbacks)} callback(s) configur√©(s) pour l'entra√Ænement"
        )

        # Prepare timeout manager with cleanup callback
        tm = None
        if timeout is not None and timeout > 0:

            def _cleanup_on_timeout():
                logger.info(
                    "[TRAINING] Timeout reached, attempting to save checkpoint before exit..."
                )
                try:
                    # Ensure checkpoint directory exists
                    os.makedirs(checkpoint_manager.checkpoint_dir, exist_ok=True)
                except Exception:
                    pass
                try:
                    optimizer = (
                        getattr(model.policy, "optimizer", None)
                        if hasattr(model, "policy")
                        else None
                    )
                    episode = getattr(model, "_episode_num", 0)
                    total_steps = getattr(model, "num_timesteps", 0)
                    checkpoint_manager.save_checkpoint(
                        model=model,
                        optimizer=optimizer,
                        episode=episode,
                        total_steps=total_steps,
                        is_final=False,
                    )
                    logger.info("[TRAINING] Checkpoint saved on timeout.")
                except Exception as e:
                    logger.error(
                        "[TRAINING] Failed to save checkpoint on timeout: %s", e
                    )

            tm = TimeoutManager(
                timeout=float(timeout), cleanup_callback=_cleanup_on_timeout
            )

        try:
            # Entra√Ænement avec gestion des interruptions
            try:
                # Calculer le nombre d'√©tapes restantes
                total_timesteps = config.get("training", {}).get(
                    "total_timesteps", 1000000
                )
                remaining_timesteps = total_timesteps - start_timesteps

                if remaining_timesteps <= 0:
                    logger.info(
                        "[TRAINING] L'entra√Ænement est d√©j√† termin√© selon le nombre d'√©tapes total configur√©."
                    )
                    return True

                logger.info(
                    "[TRAINING] D√©marrage de l'entra√Ænement pour %d √©tapes suppl√©mentaires...",
                    remaining_timesteps,
                )

                # D√©marrer l'entra√Ænement avec les callbacks
                if tm:
                    with tm.limit():
                        model.learn(
                            total_timesteps=remaining_timesteps,
                            callback=callback,  # Utilisation du CallbackList
                            reset_num_timesteps=False,  # Ne pas r√©initialiser le compteur d'√©tapes
                            progress_bar=False,  # D√©sactiv√© pour √©viter les conflits avec notre callback personnalis√©
                            tb_log_name="PPO",  # Nom pour les logs TensorBoard
                        )
                else:
                    model.learn(
                        total_timesteps=remaining_timesteps,
                        callback=callback,  # Utilisation du CallbackList
                        reset_num_timesteps=False,  # Ne pas r√©initialiser le compteur d'√©tapes
                        progress_bar=progress_bar,  # Utiliser la barre de progression configur√©e
                        tb_log_name="PPO",  # Nom pour les logs TensorBoard
                    )
            except KeyboardInterrupt:
                logger.info(
                    "[TRAINING] \nInterruption de l'utilisateur d√©tect√©e. "
                    "Sauvegarde du dernier √©tat..."
                )
                # Sauvegarder un checkpoint final
                optimizer = None
                if hasattr(model.policy, "optimizer"):
                    optimizer = model.policy.optimizer

                episode = 0
                if hasattr(model, "_episode_num"):
                    episode = model._episode_num

                total_steps = 0
                if hasattr(model, "num_timesteps"):
                    total_steps = model.num_timesteps

                checkpoint_manager.save_checkpoint(
                    model=model,
                    optimizer=optimizer,
                    episode=episode,
                    total_steps=total_steps,
                    is_final=True,
                )
                logger.info("Dernier √©tat sauvegard√© avec succ√®s.")
                raise
            logger.info("Entra√Ænement termin√© avec succ√®s!")

        except (TimeoutException, TMTimeoutException):
            logger.info("Temps d'entra√Ænement √©coul√©. Arr√™t de l'entra√Ænement...")
        except Exception as e:
            logger.error("Erreur lors de l'entra√Ænement: %s", str(e))
            raise

        finally:
            # Sauvegarder le mod√®le final
            final_metrics = {}
            if hasattr(model, "ep_info_buffer"):
                rewards = [ep_info["r"] for ep_info in model.ep_info_buffer]
                if rewards:  # V√©rifier si la liste n'est pas vide
                    final_metrics["episode_reward"] = np.mean(rewards)

            optimizer = None
            if hasattr(model.policy, "optimizer"):
                optimizer = model.policy.optimizer

            episode = 0
            if hasattr(model, "_episode_num"):
                episode = model._episode_num

            total_steps = 0
            if hasattr(model, "num_timesteps"):
                total_steps = model.num_timesteps

            checkpoint_manager.save_checkpoint(
                model=model,
                optimizer=optimizer,
                episode=episode,
                total_steps=total_steps,
                metrics=final_metrics,
                custom_data={
                    "config": config,
                    "policy_kwargs": policy_kwargs,
                    "training_complete": True,
                },
                is_final=True,
            )
            logger.info("Mod√®le final sauvegard√© avec succ√®s.")

            # Afficher un message de fin
            logger.info("Entra√Ænement termin√©. Nettoyage des ressources...")

        # Nettoyer
        env.close()
        return True

    except Exception as e:
        logger.error("Erreur lors de l'ex√©cution de l'entra√Ænement: %s", str(e))
        logger.error(traceback.format_exc())

        # Fermer les environnements en cas d'erreur
        if "env" in locals():
            env.close()

        return False


if __name__ == "__main__":
    class AttentionVisualizerCallback(BaseCallback):
        """
        Callback pour la visualisation des cartes d'attention.
        """
        def __init__(self, viz_cfg, verbose=0):
            super().__init__(verbose)
            self.viz_cfg = viz_cfg
            self.output_dir = viz_cfg.get('attention_map_dir', 'attention_maps')
            os.makedirs(self.output_dir, exist_ok=True)
            self.interval = viz_cfg.get('interval', 1000)

        def _on_step(self) -> bool:
            if self.num_timesteps % self.interval == 0:
                try:
                    # R√©cup√©rer l'environnement
                    env = self.training_env.envs[0] if hasattr(self.training_env, 'envs') else self.training_env

                    # R√©cup√©rer l'observation actuelle
                    obs = env.render(mode='rgb_array')

                    # R√©cup√©rer la carte d'attention du mod√®le
                    if hasattr(self.model.policy.features_extractor, 'get_attention_map'):
                        with torch.no_grad():
                            # Convertir l'observation en tenseur
                            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.model.device)
                            attention_map = self.model.policy.features_extractor.get_attention_map(obs_tensor)

                            # Sauvegarder la visualisation
                            output_path = os.path.join(
                                self.output_dir,
                                f'attention_step_{self.num_timesteps}.png'
                            )
                            self._save_attention_visualization(obs, attention_map, output_path)

                except Exception as e:
                    print(f"Erreur lors de la g√©n√©ration de la visualisation d'attention: {e}")

            return True

        def _save_attention_visualization(self, obs, attention_map, output_path):
            """
            Sauvegarde la visualisation de l'attention superpos√©e √† l'observation.
            """
            import matplotlib.pyplot as plt
            from matplotlib import cm
            import cv2

            # Cr√©er une figure avec deux sous-graphiques
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

            # Afficher l'observation originale
            ax1.imshow(obs)
            ax1.set_title('Observation originale')
            ax1.axis('off')

            # Afficher la carte d'attention
            if len(attention_map.shape) > 2:
                attention_map = attention_map.mean(dim=1).squeeze()

            # Redimensionner la carte d'attention pour correspondre √† l'observation
            attention_map_np = attention_map.cpu().numpy()
            resized_attention = cv2.resize(
                attention_map_np,
                (obs.shape[1], obs.shape[0]),
                interpolation=cv2.INTER_CUBIC
            )

            # Afficher la carte d'attention
            im = ax2.imshow(resized_attention, cmap='viridis')
            ax2.set_title('Carte d\'attention')
            ax2.axis('off')
            plt.colorbar(im, ax=ax2)

            # Sauvegarder la figure
            plt.tight_layout()
            plt.savefig(output_path, bbox_inches='tight', dpi=150)
            plt.close()

            if self.verbose > 0:
                print(f"Carte d'attention sauvegard√©e: {output_path}")

    parser = argparse.ArgumentParser(
        description=(
            "Entra√Æne un bot de trading ADAN avec support du timeout "
            "et des points de contr√¥le"
        )
    )
    parser.add_argument(
        "-c",
        "--config-path",
        type=str,
        default="bot/config/config.yaml",
        help="Chemin vers le fichier de configuration YAML",
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=None,
    )
    parser.add_argument(
        "--checkpoint-dir",
        type=str,
        default="checkpoints",
        help="R√©pertoire pour enregistrer les points de contr√¥le",
    )
    parser.add_argument(
        "--shared-model",
        type=str,
        default=None,
        help=("Chemin vers un mod√®le partag√© pour l'entra√Ænement distribu√©"),
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Reprend l'entra√Ænement √† partir du dernier point de contr√¥le",
    )

    # Ajout des arguments suppl√©mentaires mentionn√©s dans le patch
    parser.add_argument(
        "--workers",
        type=int,
        default=None,
        help="Number of parallel workers (optional)",
    )
    parser.add_argument("--debug", action="store_true", help="Enable debug mode")

    args = parser.parse_args()

    # Appel robuste de la fonction main()
    try:
        # 1) Appel avec les arguments nomm√©s
        success = main(
            config_path=args.config_path,
            timeout=args.timeout,
            checkpoint_dir=args.checkpoint_dir,
            shared_model_path=args.shared_model,
            resume=args.resume,
            num_envs=(args.workers if args.workers is not None else 4),
            use_subproc=True,
        )
    except Exception as e:
        print(f"Erreur lors de l'ex√©cution: {e}", file=sys.stderr)
        raise

    sys.exit(0 if success else 1)
