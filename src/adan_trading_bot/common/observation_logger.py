"""
Observation logging module for the ADAN Trading Bot.

This module provides comprehensive logging capabilities for observations,
including detailed metrics, validation results, and debugging information.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Union
import logging
import json
import time
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict
import hashlib

logger = logging.getLogger(__name__)

@dataclass
class ObservationMetrics:
    """Metrics for an observation."""
    timestamp: str
    observation_id: str
    shape: tuple
    dtype: str
    min_value: float
    max_value: float
    mean_value: float
    std_value: float
    nan_count: int
    inf_count: int
    zero_count: int
    unique_values_ratio: float
    memory_usage_mb: float
    processing_time_ms: float

@dataclass
class TimeframeMetrics:
    """Metrics for a specific timeframe within an observation."""
    timeframe: str
    shape: tuple
    min_value: float
    max_value: float
    mean_value: float
    std_value: float
    feature_stats: Dict[int, Dict[str, float]]

class ObservationLogger:
    """
    Comprehensive logger for multi-timeframe observations.
    
    This class provides detailed logging, metrics collection, and debugging
    capabilities for observations generated by the StateBuilder.
    """
    
    def __init__(self, 
                 log_dir: str = "logs/observations",
                 enable_detailed_logging: bool = True,
                 enable_metrics_collection: bool = True,
                 enable_validation_logging: bool = True,
                 log_level: str = "INFO",
                 max_log_files: int = 10):
        """
        Initialize the observation logger.
        
        Args:
            log_dir: Directory to store observation logs
            enable_detailed_logging: Whether to log detailed observation info
            enable_metrics_collection: Whether to collect and store metrics
            enable_validation_logging: Whether to log validation results
            log_level: Logging level (DEBUG, INFO, WARNING, ERROR)
            max_log_files: Maximum number of log files to keep
        """
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        self.enable_detailed_logging = enable_detailed_logging
        self.enable_metrics_collection = enable_metrics_collection
        self.enable_validation_logging = enable_validation_logging
        self.max_log_files = max_log_files
        
        # Setup logging
        self.logger = logging.getLogger(f"{__name__}.ObservationLogger")
        self.logger.setLevel(getattr(logging, log_level.upper()))
        
        # Create file handler for observation logs
        log_file = self.log_dir / f"observations_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(getattr(logging, log_level.upper()))
        
        # Create formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        
        # Add handler if not already present
        if not self.logger.handlers:
            self.logger.addHandler(file_handler)
        
        # Metrics storage
        self.metrics_history = []
        self.session_stats = {
            'total_observations': 0,
            'total_processing_time_ms': 0,
            'total_memory_usage_mb': 0,
            'validation_failures': 0,
            'start_time': datetime.now().isoformat()
        }
        
        # Clean up old log files
        self._cleanup_old_logs()
        
        self.logger.info("ObservationLogger initialized")
    
    def log_observation(self, 
                       observation: np.ndarray,
                       observation_id: Optional[str] = None,
                       context: Optional[Dict[str, Any]] = None,
                       validation_results: Optional[List] = None) -> str:
        """
        Log a complete observation with metrics and context.
        
        Args:
            observation: The observation array to log
            observation_id: Unique identifier for the observation
            context: Additional context information
            validation_results: Results from observation validation
            
        Returns:
            Generated observation ID
        """
        start_time = time.time()
        
        # Generate observation ID if not provided
        if observation_id is None:
            observation_id = self._generate_observation_id(observation)
        
        try:
            # Collect metrics
            metrics = self._collect_metrics(observation, observation_id, start_time)
            
            # Log basic information
            if self.enable_detailed_logging:
                self._log_basic_info(observation, observation_id, context)
            
            # Log detailed metrics
            if self.enable_metrics_collection:
                self._log_metrics(metrics)
                self.metrics_history.append(metrics)
            
            # Log validation results
            if self.enable_validation_logging and validation_results:
                self._log_validation_results(observation_id, validation_results)
            
            # Log timeframe-specific information
            if observation.ndim == 3:  # Multi-timeframe observation
                self._log_timeframe_details(observation, observation_id)
            
            # Update session statistics
            self._update_session_stats(metrics)
            
            self.logger.info(f"Successfully logged observation {observation_id}")
            
        except Exception as e:
            self.logger.error(f"Error logging observation {observation_id}: {e}")
        
        return observation_id
    
    def log_batch_observations(self, 
                              observations: np.ndarray,
                              batch_id: Optional[str] = None,
                              context: Optional[Dict[str, Any]] = None) -> List[str]:
        """
        Log a batch of observations.
        
        Args:
            observations: Batch of observations (batch_size, ...)
            batch_id: Unique identifier for the batch
            context: Additional context information
            
        Returns:
            List of generated observation IDs
        """
        if batch_id is None:
            batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        
        self.logger.info(f"Logging batch {batch_id} with {observations.shape[0]} observations")
        
        observation_ids = []
        
        for i, obs in enumerate(observations):
            obs_id = f"{batch_id}_obs_{i:04d}"
            obs_context = context.copy() if context else {}
            obs_context.update({
                'batch_id': batch_id,
                'batch_index': i,
                'batch_size': observations.shape[0]
            })
            
            logged_id = self.log_observation(obs, obs_id, obs_context)
            observation_ids.append(logged_id)
        
        self.logger.info(f"Completed logging batch {batch_id}")
        return observation_ids
    
    def log_state_builder_output(self, 
                                state_builder,
                                observation: np.ndarray,
                                current_idx: int,
                                data_info: Optional[Dict[str, Any]] = None) -> str:
        """
        Log observation with StateBuilder-specific context.
        
        Args:
            state_builder: StateBuilder instance
            observation: Generated observation
            current_idx: Current data index
            data_info: Information about the source data
            
        Returns:
            Generated observation ID
        """
        context = {
            'source': 'StateBuilder',
            'current_idx': current_idx,
            'window_size': state_builder.window_size,
            'normalize': state_builder.normalize,
            'timeframes': state_builder.timeframes,
            'features_config': state_builder.features_config
        }
        
        if data_info:
            context.update(data_info)
        
        # Add normalization statistics if available
        if hasattr(state_builder, 'get_normalization_stats'):
            try:
                norm_stats = state_builder.get_normalization_stats()
                context['normalization_stats'] = norm_stats
            except Exception as e:
                self.logger.warning(f"Could not get normalization stats: {e}")
        
        return self.log_observation(observation, context=context)
    
    def _collect_metrics(self, observation: np.ndarray, observation_id: str, start_time: float) -> ObservationMetrics:
        """Collect comprehensive metrics for an observation."""
        processing_time_ms = (time.time() - start_time) * 1000
        
        # Basic statistics
        flat_obs = observation.flatten()
        
        return ObservationMetrics(
            timestamp=datetime.now().isoformat(),
            observation_id=observation_id,
            shape=observation.shape,
            dtype=str(observation.dtype),
            min_value=float(np.min(observation)),
            max_value=float(np.max(observation)),
            mean_value=float(np.mean(observation)),
            std_value=float(np.std(observation)),
            nan_count=int(np.isnan(observation).sum()),
            inf_count=int(np.isinf(observation).sum()),
            zero_count=int(np.count_nonzero(observation == 0)),
            unique_values_ratio=float(len(np.unique(flat_obs)) / len(flat_obs)),
            memory_usage_mb=float(observation.nbytes / (1024 * 1024)),
            processing_time_ms=processing_time_ms
        )
    
    def _log_basic_info(self, observation: np.ndarray, observation_id: str, context: Optional[Dict[str, Any]]):
        """Log basic observation information."""
        self.logger.info(f"Observation {observation_id}:")
        self.logger.info(f"  Shape: {observation.shape}")
        self.logger.info(f"  Dtype: {observation.dtype}")
        self.logger.info(f"  Memory: {observation.nbytes / (1024 * 1024):.2f} MB")
        
        if context:
            self.logger.info(f"  Context: {json.dumps(context, indent=2, default=str)}")
    
    def _log_metrics(self, metrics: ObservationMetrics):
        """Log detailed metrics."""
        self.logger.info(f"Metrics for {metrics.observation_id}:")
        self.logger.info(f"  Value range: [{metrics.min_value:.6f}, {metrics.max_value:.6f}]")
        self.logger.info(f"  Mean: {metrics.mean_value:.6f}, Std: {metrics.std_value:.6f}")
        self.logger.info(f"  NaN count: {metrics.nan_count}, Inf count: {metrics.inf_count}")
        self.logger.info(f"  Zero count: {metrics.zero_count}")
        self.logger.info(f"  Unique values ratio: {metrics.unique_values_ratio:.4f}")
        self.logger.info(f"  Processing time: {metrics.processing_time_ms:.2f} ms")
    
    def _log_validation_results(self, observation_id: str, validation_results: List):
        """Log validation results."""
        self.logger.info(f"Validation results for {observation_id}:")
        
        for result in validation_results:
            level_str = result.level.value.upper()
            self.logger.info(f"  [{level_str}] {result.message}")
            
            if hasattr(result, 'details') and result.details:
                self.logger.debug(f"    Details: {json.dumps(result.details, indent=4, default=str)}")
    
    def _log_timeframe_details(self, observation: np.ndarray, observation_id: str):
        """Log timeframe-specific details for multi-timeframe observations."""
        if observation.ndim != 3:
            return
        
        timeframes = ['5m', '1h', '4h']  # Default timeframes
        
        self.logger.info(f"Timeframe details for {observation_id}:")
        
        for i, tf in enumerate(timeframes):
            if i < observation.shape[0]:
                tf_data = observation[i]
                tf_metrics = TimeframeMetrics(
                    timeframe=tf,
                    shape=tf_data.shape,
                    min_value=float(np.min(tf_data)),
                    max_value=float(np.max(tf_data)),
                    mean_value=float(np.mean(tf_data)),
                    std_value=float(np.std(tf_data)),
                    feature_stats={}
                )
                
                # Collect per-feature statistics
                for feature_idx in range(tf_data.shape[1]):
                    feature_data = tf_data[:, feature_idx]
                    tf_metrics.feature_stats[feature_idx] = {
                        'min': float(np.min(feature_data)),
                        'max': float(np.max(feature_data)),
                        'mean': float(np.mean(feature_data)),
                        'std': float(np.std(feature_data))
                    }
                
                self.logger.info(f"  {tf}: shape={tf_metrics.shape}, "
                               f"range=[{tf_metrics.min_value:.4f}, {tf_metrics.max_value:.4f}], "
                               f"mean={tf_metrics.mean_value:.4f}, std={tf_metrics.std_value:.4f}")
                
                # Log feature details at debug level
                self.logger.debug(f"    Feature stats: {json.dumps(tf_metrics.feature_stats, indent=6)}")
    
    def _update_session_stats(self, metrics: ObservationMetrics):
        """Update session-level statistics."""
        self.session_stats['total_observations'] += 1
        self.session_stats['total_processing_time_ms'] += metrics.processing_time_ms
        self.session_stats['total_memory_usage_mb'] += metrics.memory_usage_mb
        
        if metrics.nan_count > 0 or metrics.inf_count > 0:
            self.session_stats['validation_failures'] += 1
    
    def _generate_observation_id(self, observation: np.ndarray) -> str:
        """Generate a unique ID for an observation based on its content."""
        # Create a hash of the observation content and timestamp
        content_hash = hashlib.md5(observation.tobytes()).hexdigest()[:8]
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]  # Remove last 3 digits of microseconds
        return f"obs_{timestamp}_{content_hash}"
    
    def _cleanup_old_logs(self):
        """Clean up old log files to maintain max_log_files limit."""
        try:
            log_files = list(self.log_dir.glob("observations_*.log"))
            if len(log_files) > self.max_log_files:
                # Sort by modification time and remove oldest
                log_files.sort(key=lambda x: x.stat().st_mtime)
                for old_file in log_files[:-self.max_log_files]:
                    old_file.unlink()
                    self.logger.info(f"Removed old log file: {old_file}")
        except Exception as e:
            self.logger.warning(f"Error cleaning up old logs: {e}")
    
    def get_session_summary(self) -> Dict[str, Any]:
        """Get a summary of the current logging session."""
        current_time = datetime.now()
        start_time = datetime.fromisoformat(self.session_stats['start_time'])
        session_duration = (current_time - start_time).total_seconds()
        
        summary = self.session_stats.copy()
        summary.update({
            'session_duration_seconds': session_duration,
            'observations_per_second': self.session_stats['total_observations'] / max(session_duration, 1),
            'average_processing_time_ms': (
                self.session_stats['total_processing_time_ms'] / 
                max(self.session_stats['total_observations'], 1)
            ),
            'average_memory_usage_mb': (
                self.session_stats['total_memory_usage_mb'] / 
                max(self.session_stats['total_observations'], 1)
            ),
            'validation_failure_rate': (
                self.session_stats['validation_failures'] / 
                max(self.session_stats['total_observations'], 1)
            )
        })
        
        return summary
    
    def export_metrics(self, output_file: Optional[str] = None) -> str:
        """Export collected metrics to a JSON file."""
        if output_file is None:
            output_file = self.log_dir / f"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        else:
            output_file = Path(output_file)
        
        export_data = {
            'session_summary': self.get_session_summary(),
            'metrics_history': [asdict(m) for m in self.metrics_history]
        }
        
        with open(output_file, 'w') as f:
            json.dump(export_data, f, indent=2, default=str)
        
        self.logger.info(f"Exported metrics to {output_file}")
        return str(output_file)
    
    def reset_session(self):
        """Reset session statistics and metrics history."""
        self.metrics_history.clear()
        self.session_stats = {
            'total_observations': 0,
            'total_processing_time_ms': 0,
            'total_memory_usage_mb': 0,
            'validation_failures': 0,
            'start_time': datetime.now().isoformat()
        }
        self.logger.info("Session statistics reset")
    
    def close(self):
        """Close the logger and export final metrics."""
        try:
            # Export final metrics
            self.export_metrics()
            
            # Log session summary
            summary = self.get_session_summary()
            self.logger.info("Final session summary:")
            self.logger.info(json.dumps(summary, indent=2, default=str))
            
            # Close handlers
            for handler in self.logger.handlers:
                handler.close()
                self.logger.removeHandler(handler)
                
        except Exception as e:
            logger.error(f"Error closing ObservationLogger: {e}")