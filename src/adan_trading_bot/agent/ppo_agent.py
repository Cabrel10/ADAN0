#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""Agent PPO avec callback d'affichage hi√©rarchique pour ADAN Trading Bot."""

import logging
import os
import time
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.logger import configure as sb3_configure


def setup_logger(log_file: str = "training_log.txt", logger_name: str = 'PPOTraining') -> logging.Logger:
    """
    Configuration du logger pour l'entra√Ænement PPO.

    Args:
        log_file: Chemin vers le fichier de log
        logger_name: Nom du logger

    Returns:
        Logger configur√©
    """
    logger = logging.getLogger(logger_name)

    # √âviter la duplication des handlers
    if logger.handlers:
        return logger

    logger.setLevel(logging.INFO)

    # Handler pour la console
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)

    # Handler pour le fichier
    fh = logging.FileHandler(log_file, encoding='utf-8')
    fh.setLevel(logging.INFO)

    # Format des messages
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    ch.setFormatter(formatter)
    fh.setFormatter(formatter)

    logger.addHandler(ch)
    logger.addHandler(fh)

    return logger


class HierarchicalTrainingDisplayCallback(BaseCallback):
    """
    Callback personnalis√© pour un affichage hi√©rarchique complet de l'entra√Ænement.
    Affiche les m√©triques de portfolio, positions, m√©triques financi√®res et mod√®le.
    """

    def __init__(
        self,
        verbose: int = 0,
        display_freq: int = 1000,
        total_timesteps: int = 1000000,
        initial_capital: float = 20.50,
        log_file: str = "training_log.txt"
    ):
        """
        Initialise le callback d'affichage hi√©rarchique.

        Args:
            verbose: Niveau de verbosit√©
            display_freq: Fr√©quence d'affichage (en steps)
            total_timesteps: Nombre total de timesteps pour l'entra√Ænement
            initial_capital: Capital initial du portfolio
            log_file: Fichier de log
        """
        super(HierarchicalTrainingDisplayCallback, self).__init__(verbose)
        self.display_freq = display_freq
        self.total_timesteps = total_timesteps
        self.initial_capital = initial_capital
        self.correlation_id = str(uuid.uuid4())[:8]

        # Configurer le logger
        self.logger = setup_logger(log_file)

        # M√©triques de suivi
        self.episode_rewards = []
        self.episode_count = 0
        self.start_time = time.time()
        self.last_display_time = time.time()

        # M√©triques financi√®res
        self.metrics = {
            "sharpe": 0.0,
            "sortino": 0.0,
            "profit_factor": 0.0,
            "max_dd": 0.0,
            "cagr": 0.0,
            "win_rate": 0.0,
            "trades": 0,
            "volatility": 0.0
        }

        # Positions et portfolio
        self.positions = {}
        self.closed_positions = []
        self.portfolio_value = initial_capital
        self.drawdown = 0.0
        self.cash = initial_capital

    def _on_training_start(self) -> None:
        """D√©marrage de l'entra√Ænement avec affichage de la configuration."""
        self.start_time = time.time()
        self.last_display_time = self.start_time

        self.logger.info("‚ï≠" + "‚îÄ" * 70 + "‚ïÆ")
        self.logger.info("‚îÇ" + " " * 20 + "üöÄ D√âMARRAGE ADAN TRAINING" + " " * 20 + "‚îÇ")
        self.logger.info("‚ï∞" + "‚îÄ" * 70 + "‚ïØ")

        self.logger.info(f"[TRAINING START] Correlation ID: {self.correlation_id}")
        self.logger.info(f"[TRAINING START] Total timesteps: {self.total_timesteps:,}")
        self.logger.info(f"[TRAINING START] Display frequency: {self.display_freq:,} steps")
        self.logger.info(f"[TRAINING START] Capital initial: ${self.initial_capital:.2f}")

        # Section Configuration Flux Mon√©taires
        self.logger.info("‚ï≠" + "‚îÄ" * 25 + " Configuration Flux Mon√©taires " + "‚îÄ" * 25 + "‚ïÆ")
        self.logger.info(f"‚îÇ üí∞ Capital Initial: ${self.initial_capital:<40.2f}‚îÇ")
        self.logger.info("‚îÇ üéØ Gestion Dynamique des Flux Activ√©e" + " " * 32 + "‚îÇ")
        self.logger.info("‚îÇ üìä Monitoring en Temps R√©el" + " " * 39 + "‚îÇ")
        self.logger.info("‚îÇ üîÑ Corr√©lation ID: " + self.correlation_id + " " * 42 + "‚îÇ")
        self.logger.info("‚ï∞" + "‚îÄ" * 82 + "‚ïØ")

    def _on_step(self) -> bool:
        """
        Appel√© √† chaque √©tape pour mettre √† jour l'affichage et collecter les m√©triques.
        """
        # Collecter les r√©compenses depuis les informations locales
        try:
            # R√©cup√©rer les r√©compenses depuis les buffers du mod√®le
            if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
                recent_episodes = self.model.ep_info_buffer[-10:]  # 10 derniers √©pisodes
                rewards = [ep_info.get('r', 0) for ep_info in recent_episodes if 'r' in ep_info]
                if rewards:
                    self.episode_rewards.extend(rewards[-5:])  # Garder les 5 derni√®res r√©compenses
                    if len(self.episode_rewards) > 50:  # Limiter la taille du buffer
                        self.episode_rewards = self.episode_rewards[-50:]
        except Exception as e:
            self.logger.debug(f"Erreur lors de la collecte des r√©compenses: {e}")

        # V√©rifier si un √©pisode est termin√© et afficher la progression
        if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
            current_episodes = len(self.model.ep_info_buffer)
            if current_episodes > self.episode_count:
                self.episode_count = current_episodes
                self._display_episode_progress()

        # Affichage hi√©rarchique p√©riodique
        if self.num_timesteps % self.display_freq == 0 and self.num_timesteps > 0:
            self._log_detailed_metrics()

        return True

    def _display_episode_progress(self) -> None:
        """Affiche la barre de progression √† la fin de chaque √©pisode."""
        try:
            progress = (self.num_timesteps / self.total_timesteps) * 100

            # Barre de progression visuelle
            progress_bar_length = 30
            filled_length = int(progress_bar_length * progress / 100)
            bar = "‚îÅ" * filled_length + "‚îÄ" * (progress_bar_length - filled_length)

            # Calculer la r√©compense moyenne r√©cente
            mean_reward = 0.0
            if self.episode_rewards:
                recent_rewards = self.episode_rewards[-10:]
                mean_reward = np.mean(recent_rewards)

            # Temps √©coul√© et ETA
            elapsed = time.time() - self.start_time
            if progress > 0:
                eta = (elapsed / progress * 100) - elapsed
                eta_str = str(timedelta(seconds=int(eta)))
            else:
                eta_str = "N/A"

            self.logger.info(
                f"üöÄ ADAN Training {bar} {progress:.1f}% ({self.num_timesteps:,}/{self.total_timesteps:,}) ‚Ä¢ "
                f"Episode {self.episode_count} ‚Ä¢ Mean Reward: {mean_reward:.3f} ‚Ä¢ ETA: {eta_str}"
            )

        except Exception as e:
            self.logger.error(f"Erreur lors de l'affichage de la progression: {e}")

    def _log_detailed_metrics(self) -> None:
        """Affichage d√©taill√© des m√©triques avec structure hi√©rarchique."""
        try:
            self._update_environment_metrics()
            self._update_model_metrics()

            # Temps et vitesse
            elapsed = time.time() - self.start_time
            steps_per_sec = self.num_timesteps / elapsed if elapsed > 0 else 0.0
            current_time = time.time()
            recent_steps_per_sec = self.display_freq / (current_time - self.last_display_time) if current_time > self.last_display_time else 0.0
            self.last_display_time = current_time

            # En-t√™te de section
            self.logger.info("‚ï≠" + "‚îÄ" * 80 + "‚ïÆ")
            self.logger.info("‚îÇ" + " " * 25 + f"√âTAPE {self.num_timesteps:,}" + " " * 25 + "‚îÇ")
            self.logger.info("‚ï∞" + "‚îÄ" * 80 + "‚ïØ")

            # M√©triques de portfolio
            roi = ((self.portfolio_value - self.initial_capital) / self.initial_capital) * 100 if self.initial_capital > 0 else 0
            self.logger.info(
                f"üìä PORTFOLIO | Valeur: ${self.portfolio_value:.2f} | Cash: ${self.cash:.2f} | "
                f"ROI: {roi:+.2f}%"
            )

            # M√©triques de risque
            self.logger.info(
                f"‚ö†Ô∏è  RISK | Drawdown: {self.drawdown:.2f}% | Max DD: {self.metrics['max_dd']:.2f}% | "
                f"Volatilit√©: {self.metrics['volatility']:.2f}%"
            )

            # M√©triques de performance
            self.logger.info(
                f"üìà METRICS | Sharpe: {self.metrics['sharpe']:.2f} | Sortino: {self.metrics['sortino']:.2f} | "
                f"Profit Factor: {self.metrics['profit_factor']:.2f}"
            )

            self.logger.info(
                f"üìä TRADING | CAGR: {self.metrics['cagr']:.2f}% | Win Rate: {self.metrics['win_rate']:.1f}% | "
                f"Trades: {self.metrics['trades']}"
            )

            # Positions ouvertes
            self._display_open_positions()

            # M√©triques du mod√®le
            self._display_model_metrics()

            # Informations temporelles
            self.logger.info(
                f"‚è±Ô∏è  TIMING | Elapsed: {elapsed/60:.1f}min | Speed: {steps_per_sec:.1f} steps/s | "
                f"Recent: {recent_steps_per_sec:.1f} steps/s"
            )

            self.logger.info("‚îÄ" * 80)

        except Exception as e:
            self.logger.error(f"Erreur lors de l'affichage des m√©triques d√©taill√©es: {e}")

    def _update_environment_metrics(self) -> None:
        """Met √† jour les m√©triques depuis l'environnement."""
        try:
            # Essayer de r√©cup√©rer les m√©triques via l'environnement vectoris√©
            if hasattr(self.model, 'get_env'):
                env = self.model.get_env()

                # Pour les environnements vectoris√©s
                if hasattr(env, 'envs') and len(env.envs) > 0:
                    first_env = env.envs[0]

                    # Naviguer √† travers les wrappers pour trouver les m√©triques
                    current_env = first_env
                    while hasattr(current_env, 'env'):
                        if hasattr(current_env, 'get_portfolio_metrics'):
                            metrics = current_env.get_portfolio_metrics()
                            if metrics:
                                self.portfolio_value = metrics.get('portfolio_value', self.initial_capital)
                                self.drawdown = metrics.get('drawdown', 0.0)
                                self.cash = metrics.get('cash', self.initial_capital)
                                self.positions = metrics.get('positions', {})
                                self.closed_positions = metrics.get('closed_positions', [])

                                # Mettre √† jour les m√©triques financi√®res
                                self.metrics.update({
                                    "sharpe": metrics.get('sharpe', 0.0),
                                    "sortino": metrics.get('sortino', 0.0),
                                    "profit_factor": metrics.get('profit_factor', 0.0),
                                    "max_dd": metrics.get('max_dd', 0.0),
                                    "cagr": metrics.get('cagr', 0.0),
                                    "win_rate": metrics.get('win_rate', 0.0),
                                    "trades": metrics.get('trades', 0),
                                    "volatility": metrics.get('volatility', 0.0)
                                })
                                break
                        current_env = getattr(current_env, 'env', None)
                        if current_env is None:
                            break

                # M√©thode de fallback avec get_attr si disponible
                elif hasattr(env, 'get_attr'):
                    try:
                        env_infos = env.get_attr('last_info')
                        if env_infos and len(env_infos) > 0 and env_infos[0]:
                            info = env_infos[0]
                            self._extract_info_metrics(info)
                    except Exception as e:
                        self.logger.debug(f"Fallback get_attr failed: {e}")

        except Exception as e:
            self.logger.debug(f"Erreur lors de la mise √† jour des m√©triques d'environnement: {e}")

    def _extract_info_metrics(self, info: Dict[str, Any]) -> None:
        """Extrait les m√©triques depuis le dictionnaire info."""
        self.portfolio_value = info.get('portfolio_value', self.initial_capital)
        self.drawdown = info.get('drawdown', 0.0)
        self.cash = info.get('cash', self.initial_capital)
        self.positions = info.get('positions', {})
        self.closed_positions = info.get('closed_positions', [])

        # Mettre √† jour les m√©triques financi√®res
        self.metrics.update({
            "sharpe": info.get('sharpe', 0.0),
            "sortino": info.get('sortino', 0.0),
            "profit_factor": info.get('profit_factor', 0.0),
            "max_dd": info.get('max_dd', 0.0),
            "cagr": info.get('cagr', 0.0),
            "win_rate": info.get('win_rate', 0.0),
            "trades": info.get('trades', 0),
            "volatility": info.get('volatility', 0.0)
        })

    def _update_model_metrics(self) -> None:
        """Met √† jour les m√©triques du mod√®le PPO."""
        try:
            self.model_metrics = {}
            if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
                self.model_metrics = self.model.logger.name_to_value
        except Exception as e:
            self.logger.debug(f"Erreur lors de la mise √† jour des m√©triques du mod√®le: {e}")

    def _display_open_positions(self) -> None:
        """Affiche les positions ouvertes si disponibles."""
        try:
            if self.positions and any(self.positions.values()):
                self.logger.info("‚ï≠" + "‚îÄ" * 28 + " Positions Ouvertes " + "‚îÄ" * 28 + "‚ïÆ")

                for asset, pos in self.positions.items():
                    if isinstance(pos, dict) and pos:
                        size = pos.get('size', 0)
                        entry_price = pos.get('entry_price', 0)
                        current_price = pos.get('current_price', entry_price)
                        value = pos.get('value', 0)
                        sl = pos.get('sl', 0)
                        tp = pos.get('tp', 0)
                        pnl_unrealized = pos.get('pnl_unrealized', 0)

                        self.logger.info(
                            f"‚îÇ {asset}: Size: {size:.2f} @ {entry_price:.4f} | "
                            f"Current: {current_price:.4f} | Value: ${value:.2f}"
                            + " " * (80 - len(f"‚îÇ {asset}: Size: {size:.2f} @ {entry_price:.4f} | Current: {current_price:.4f} | Value: ${value:.2f}")) + "‚îÇ"
                        )

                        if sl > 0 or tp > 0:
                            self.logger.info(
                                f"‚îÇ   ‚îî‚îÄ SL: {sl:.4f} | TP: {tp:.4f} | P&L: ${pnl_unrealized:.2f}"
                                + " " * (80 - len(f"‚îÇ   ‚îî‚îÄ SL: {sl:.4f} | TP: {tp:.4f} | P&L: ${pnl_unrealized:.2f}")) + "‚îÇ"
                            )

                self.logger.info("‚ï∞" + "‚îÄ" * 78 + "‚ïØ")
            else:
                self.logger.info("üìù POSITIONS | Aucune position ouverte")
        except Exception as e:
            self.logger.error(f"Erreur lors de l'affichage des positions: {e}")

    def _display_model_metrics(self) -> None:
        """Affiche les m√©triques du mod√®le PPO."""
        try:
            if hasattr(self, 'model_metrics') and self.model_metrics:
                total_loss = self.model_metrics.get("train/loss", 0.0)
                policy_loss = self.model_metrics.get("train/policy_loss", 0.0)
                value_loss = self.model_metrics.get("train/value_loss", 0.0)
                entropy = self.model_metrics.get("train/entropy_loss", 0.0)
                clip_fraction = self.model_metrics.get("train/clip_fraction", 0.0)

                self.logger.info(
                    f"üß† MODEL | Loss: {total_loss:.4f} | Policy: {policy_loss:.4f} | "
                    f"Value: {value_loss:.4f} | Entropy: {entropy:.4f}"
                )

                if clip_fraction > 0:
                    self.logger.info(f"üéØ LEARNING | Clip Fraction: {clip_fraction:.3f}")
            else:
                self.logger.info("üß† MODEL | M√©triques non disponibles")
        except Exception as e:
            self.logger.error(f"Erreur lors de l'affichage des m√©triques du mod√®le: {e}")

    def _on_rollout_end(self) -> None:
        """Appel√© √† la fin de chaque rollout pour capturer les positions ferm√©es."""
        try:
            self._display_closed_positions()
        except Exception as e:
            self.logger.error(f"Erreur lors du traitement du rollout: {e}")

    def _display_closed_positions(self) -> None:
        """Affiche les positions ferm√©es r√©cemment."""
        try:
            if self.closed_positions:
                recent_closed = self.closed_positions[-5:]  # 5 derni√®res positions ferm√©es

                if recent_closed:
                    self.logger.info("‚ï≠" + "‚îÄ" * 28 + " Positions Ferm√©es " + "‚îÄ" * 28 + "‚ïÆ")

                    for pos in recent_closed:
                        if isinstance(pos, dict):
                            asset = pos.get('asset', 'Unknown')
                            size = pos.get('size', 0)
                            entry_price = pos.get('entry_price', 0)
                            exit_price = pos.get('exit_price', 0)
                            pnl = pos.get('pnl', 0)
                            pnl_pct = pos.get('pnl_pct', 0)
                            duration = pos.get('duration_minutes', 0)

                            status_emoji = "üü¢" if pnl > 0 else "üî¥"

                            self.logger.info(
                                f"‚îÇ {status_emoji} {asset}: Size: {size:.2f} | "
                                f"Entry: {entry_price:.4f} | Exit: {exit_price:.4f}"
                                + " " * (80 - len(f"‚îÇ {status_emoji} {asset}: Size: {size:.2f} | Entry: {entry_price:.4f} | Exit: {exit_price:.4f}")) + "‚îÇ"
                            )

                            self.logger.info(
                                f"‚îÇ   ‚îî‚îÄ P&L: ${pnl:.2f} ({pnl_pct:+.2f}%) | Duration: {duration}min"
                                + " " * (80 - len(f"‚îÇ   ‚îî‚îÄ P&L: ${pnl:.2f} ({pnl_pct:+.2f}%) | Duration: {duration}min")) + "‚îÇ"
                            )

                    self.logger.info("‚ï∞" + "‚îÄ" * 78 + "‚ïØ")

                    # Clear the closed positions after displaying
                    self.closed_positions = []
        except Exception as e:
            self.logger.error(f"Erreur lors de l'affichage des positions ferm√©es: {e}")

    def _on_training_end(self) -> None:
        """Fin de l'entra√Ænement avec r√©sum√© complet."""
        try:
            elapsed = time.time() - self.start_time

            self.logger.info("‚ï≠" + "‚îÄ" * 70 + "‚ïÆ")
            self.logger.info("‚îÇ" + " " * 20 + "‚úÖ ENTRA√éNEMENT TERMIN√â" + " " * 20 + "‚îÇ")
            self.logger.info("‚ï∞" + "‚îÄ" * 70 + "‚ïØ")

            self.logger.info(f"[TRAINING END] Correlation ID: {self.correlation_id}")
            self.logger.info(f"[TRAINING END] Total steps: {self.num_timesteps:,}")
            self.logger.info(f"[TRAINING END] Duration: {elapsed/60:.1f} minutes ({elapsed/3600:.1f} hours)")
            self.logger.info(f"[TRAINING END] Episodes completed: {self.episode_count}")

            # R√©sum√© final des performances
            if self.episode_rewards:
                final_reward = np.mean(self.episode_rewards[-10:]) if len(self.episode_rewards) >= 10 else np.mean(self.episode_rewards)
                max_reward = np.max(self.episode_rewards)
                min_reward = np.min(self.episode_rewards)

                self.logger.info(f"[TRAINING END] Final Mean Reward: {final_reward:.3f}")
                self.logger.info(f"[TRAINING END] Best Episode Reward: {max_reward:.3f}")
                self.logger.info(f"[TRAINING END] Worst Episode Reward: {min_reward:.3f}")

            # R√©sum√© du portfolio
            final_roi = ((self.portfolio_value - self.initial_capital) / self.initial_capital) * 100 if self.initial_capital > 0 else 0
            self.logger.info(f"[TRAINING END] Final Portfolio Value: ${self.portfolio_value:.2f}")
            self.logger.info(f"[TRAINING END] Final ROI: {final_roi:+.2f}%")
            self.logger.info(f"[TRAINING END] Max Drawdown: {self.metrics['max_dd']:.2f}%")
            self.logger.info(f"[TRAINING END] Total Trades: {self.metrics['trades']}")

            # Stats de performance
            avg_steps_per_sec = self.num_timesteps / elapsed if elapsed > 0 else 0
            self.logger.info(f"[TRAINING END] Average Speed: {avg_steps_per_sec:.1f} steps/second")

        except Exception as e:
            self.logger.error(f"Erreur lors du r√©sum√© final: {e}")


class PPOAgent:
    """
    Agent PPO avec callback d'affichage hi√©rarchique int√©gr√©.
    """

    def __init__(self, env, config: Dict[str, Any]):
        """
        Initialise l'agent PPO avec la configuration donn√©e.

        Args:
            env: Environnement d'entra√Ænement
            config: Configuration de l'agent
        """
        self.env = env
        self.config = config

        # Configuration du r√©seau de neurones
        policy_kwargs = {
            "net_arch": config.get("net_arch", [256, 128]),
            "features_dim": config.get("features_dim", 256),
            "activation_fn": torch.nn.ReLU,
            "ortho_init": True
        }

        # Configuration PPO
        ppo_params = config.get("ppo_params", {})
        default_ppo_params = {
            "learning_rate": 3e-4,
            "n_steps": 2048,
            "batch_size": 64,
            "n_epochs": 10,
            "gamma": 0.99,
            "gae_lambda": 0.95,
            "clip_range": 0.2,
            "ent_coef": 0.0,
            "vf_coef": 0.5,
            "max_grad_norm": 0.5,
        }

        # Fusionner les param√®tres par d√©faut avec ceux fournis
        final_ppo_params = {**default_ppo_params, **ppo_params}

        # Cr√©er le mod√®le PPO
        self.model = PPO(
            policy="MultiInputPolicy",
            env=env,
            policy_kwargs=policy_kwargs,
            verbose=0,
            seed=config.get('seed', 42),
            device='auto',
            **final_ppo_params
        )

        # Configurer le logger SB3 si sp√©cifi√©
        if config.get('enable_sb3_logging', True):
            log_dir = config.get('log_dir', 'logs/sb3')
            Path(log_dir).mkdir(parents=True, exist_ok=True)
            new_logger = sb3_configure(str(log_dir), ["stdout", "csv", "tensorboard"])
            self.model.set_logger(new_logger)

    def learn(
        self,
        total_timesteps: int,
        log_interval: int = 1000,
        initial_capital: float = 20.50,
        callback=None,
        **kwargs
    ):
        """
        Lance l'entra√Ænement avec le callback hi√©rarchique.

        Args:
            total_timesteps: Nombre total de timesteps
            log_interval: Intervalle d'affichage des logs
            initial_capital: Capital initial du portfolio
            callback: Callback additionnel (optionnel)
            **kwargs: Arguments suppl√©mentaires pour model.learn()
        """
        # Cr√©er le callback hi√©rarchique
        hierarchical_callback = HierarchicalTrainingDisplayCallback(
            display_freq=log_interval,
            total_timesteps=total_timesteps,
            initial_capital=initial_capital,
            log_file=self.config.get('log_file', 'training_log.txt')
        )

        # Combiner avec d'autres callbacks si fournis
        if callback is not None:
            from stable_baselines3.common.callbacks import CallbackList
            if isinstance(callback, list):
                all_callbacks = [hierarchical_callback] + callback
            else:
                all_callbacks = [hierarchical_callback, callback]
            final_callback = CallbackList(all_callbacks)
        else:
            final_callback = hierarchical_callback

        # Lancer l'entra√Ænement
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=final_callback,
            progress_bar=False,  # Utiliser notre callback personnalis√©
            **kwargs
        )

        return self.model

    def predict(self, observation, **kwargs):
        """Pr√©diction avec le mod√®le entra√Æn√©."""
        return self.model.predict(observation, **kwargs)

    def save(self, path: str):
        """Sauvegarde le mod√®le."""
        self.model.save(path)

    @classmethod
    def load(cls, path: str, env=None, **kwargs):
        """Charge un mod√®le sauvegard√©."""
        model = PPO.load(path, env=env, **kwargs)

        # Cr√©er une instance de la classe avec le mod√®le charg√©
        agent = cls.__new__(cls)
        agent.model = model
        agent.env = env
        agent.config = {}

        return agent


# Exemple d'utilisation
if __name__ == "__main__":
    """Exemple d'utilisation de l'agent PPO avec callback hi√©rarchique."""
    import gym
