#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Tests d'int√©gration pour le trading bot ADAN.

Ce package contient les tests d'int√©gration pour valider le fonctionnement
du syst√®me complet et les interactions entre composants :

- test_phase1_complete_system.py: Tests d'int√©gration Phase 1 complets
- Futures tests pour workflows complets, performance, stress testing

Les tests d'int√©gration se concentrent sur :
- Flux de donn√©es complet (data loading ‚Üí processing ‚Üí trading)
- Int√©gration entre portfolio manager, data loader, et environment
- Validation des configurations complexes
- Performance du syst√®me sous charge
- Coh√©rence des r√©sultats √† travers les composants
- Scenarios de trading r√©alistes
"""

__version__ = "1.0.0"

# Importations des modules de test d'int√©gration
try:
    from .test_phase1_complete_system import (
        TestPhase1CompleteIntegration,
        TestDataIntegration
    )
except ImportError:
    # G√©rer gracieusement si les modules ne sont pas encore disponibles
    pass

# Importations pour tests d'int√©gration
import logging
import time
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

# Configuration des tests d'int√©gration
INTEGRATION_TEST_CONFIG = {
    # Sc√©narios de capital pour tests complets
    "capital_scenarios": [
        {"amount": 20.0, "tier": "Micro Capital", "description": "D√©butant avec capital minimal"},
        {"amount": 75.0, "tier": "Small Capital", "description": "Petit investisseur"},
        {"amount": 300.0, "tier": "Medium Capital", "description": "Investisseur interm√©diaire"},
        {"amount": 1200.0, "tier": "Large Capital", "description": "Investisseur avanc√©"},
        {"amount": 5000.0, "tier": "Enterprise", "description": "Investisseur institutionnel"}
    ],

    # Assets pour tests d'int√©gration complets
    "comprehensive_assets": [
        "BTCUSDT",  # Crypto majeur stable
        "ETHUSDT",  # Crypto majeur avec plus de volatilit√©
        "SOLUSDT",  # Altcoin populaire volatile
        "ADAUSDT",  # Altcoin avec diff√©rents patterns
        "XRPUSDT"   # Altcoin avec r√©gulations
    ],

    # Timeframes pour tests multi-√©chelle
    "timeframe_hierarchy": {
        "scalping": "1m",
        "short_term": "5m",
        "medium_term": "1h",
        "long_term": "4h",
        "position": "1d"
    },

    # Sc√©narios de march√© pour stress testing
    "market_scenarios": {
        "bull_market": {"trend": 1, "volatility": 0.02, "description": "March√© haussier stable"},
        "bear_market": {"trend": -1, "volatility": 0.025, "description": "March√© baissier"},
        "sideways": {"trend": 0, "volatility": 0.015, "description": "March√© lat√©ral"},
        "high_volatility": {"trend": 0, "volatility": 0.08, "description": "Haute volatilit√©"},
        "crash": {"trend": -3, "volatility": 0.15, "description": "Crash market"}
    }
}

# M√©triques de performance pour validation
PERFORMANCE_BENCHMARKS = {
    "cvar_calculation_max_time_ms": 100,  # Temps max pour calcul CVaR
    "tier_detection_max_time_ms": 10,     # Temps max pour d√©tection palier
    "normalization_max_time_ms": 5,       # Temps max pour normalisation
    "complete_flow_max_time_ms": 1000,    # Temps max pour flux complet
    "memory_usage_max_mb": 100,           # Usage m√©moire maximum acceptable
    "cpu_usage_max_percent": 80           # Usage CPU maximum acceptable
}

# Seuils de validation pour les tests d'int√©gration
INTEGRATION_VALIDATION_THRESHOLDS = {
    "position_size_variance_max": 0.20,   # Variance max 20% entre calculs identiques
    "tier_transition_smooth_factor": 1.5, # Facteur de lissage entre paliers
    "multi_timeframe_coherence_min": 0.7, # Coh√©rence min entre timeframes
    "asset_selection_stability": 0.8,     # Stabilit√© s√©lection d'actifs
    "risk_management_compliance": 1.0,    # Compliance 100% aux r√®gles de risque
    "sharpe_momentum_correlation": 0.6    # Corr√©lation min avec performance r√©elle
}

def create_integration_test_environment(capital: float, assets: List[str],
                                      market_scenario: str = "bull_market") -> Dict[str, Any]:
    """
    Cr√©e un environnement de test d'int√©gration complet.

    Args:
        capital: Capital initial pour le test
        assets: Liste des actifs √† inclure
        market_scenario: Sc√©nario de march√© √† simuler

    Returns:
        Environnement de test configur√©
    """
    scenario = INTEGRATION_TEST_CONFIG["market_scenarios"][market_scenario]

    # Configuration de l'environnement
    env_config = {
        "capital": capital,
        "assets": assets,
        "market_scenario": scenario,
        "test_duration_steps": 1000,
        "data_points_per_asset": 5000,
        "performance_tracking": True,
        "detailed_logging": True
    }

    # Configuration des paliers (copie de la config principale)
    env_config["capital_tiers"] = INTEGRATION_TEST_CONFIG.get("capital_tiers", [])

    return env_config

def simulate_realistic_market_data(assets: List[str], n_periods: int = 5000,
                                 scenario: str = "bull_market") -> Dict[str, pd.DataFrame]:
    """
    Simule des donn√©es de march√© r√©alistes pour les tests d'int√©gration.

    Args:
        assets: Liste des actifs √† simuler
        n_periods: Nombre de p√©riodes √† g√©n√©rer
        scenario: Sc√©nario de march√©

    Returns:
        Dictionnaire avec DataFrames de donn√©es de march√© par actif
    """
    market_config = INTEGRATION_TEST_CONFIG["market_scenarios"][scenario]
    trend = market_config["trend"]
    volatility = market_config["volatility"]

    market_data = {}

    # Prix de base par actif (r√©alistes)
    base_prices = {
        "BTCUSDT": 45000,
        "ETHUSDT": 3000,
        "SOLUSDT": 100,
        "ADAUSDT": 0.5,
        "XRPUSDT": 0.6
    }

    for asset in assets:
        np.random.seed(hash(asset) % 2**32)  # Seed reproductible par actif

        base_price = base_prices.get(asset, 100)

        # G√©n√©rer des rendements avec trend et volatilit√©
        daily_trend = trend * 0.001  # 0.1% par p√©riode si trend = 1
        returns = np.random.normal(daily_trend, volatility, n_periods)

        # Ajouter de la persistance (momentum effect)
        for i in range(1, len(returns)):
            returns[i] += 0.15 * returns[i-1]  # Autocorr√©lation

        # Ajouter des √©v√©nements extr√™mes
        extreme_events = np.random.random(n_periods) < 0.02  # 2% chance
        returns[extreme_events] *= 3

        # Calculer les prix
        prices = base_price * np.exp(np.cumsum(returns))

        # Cr√©er OHLCV data
        high_low_spread = volatility * 0.5
        volume_base = np.random.lognormal(15, 1, n_periods)

        timestamps = pd.date_range(start='2023-01-01', periods=n_periods, freq='1H')

        df = pd.DataFrame({
            'timestamp': timestamps,
            'open': prices * np.random.uniform(0.999, 1.001, n_periods),
            'high': prices * (1 + np.random.uniform(0, high_low_spread, n_periods)),
            'low': prices * (1 - np.random.uniform(0, high_low_spread, n_periods)),
            'close': prices,
            'volume': volume_base,
            'returns': returns
        })

        # Calculer indicateurs techniques
        df['volatility'] = df['returns'].rolling(20).std() * np.sqrt(24*365)
        df['sma_20'] = df['close'].rolling(20).mean()
        df['sma_50'] = df['close'].rolling(50).mean()

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # MACD
        ema12 = df['close'].ewm(span=12).mean()
        ema26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema12 - ema26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()

        market_data[asset] = df.dropna()

    return market_data

def measure_integration_performance(func, *args, **kwargs) -> Tuple[Any, Dict[str, float]]:
    """
    Mesure les performances d'une fonction d'int√©gration.

    Args:
        func: Fonction √† mesurer
        *args, **kwargs: Arguments de la fonction

    Returns:
        Tuple (r√©sultat, m√©triques de performance)
    """
    import psutil
    import tracemalloc

    # D√©marrer le monitoring
    tracemalloc.start()
    process = psutil.Process()
    cpu_before = process.cpu_percent()

    start_time = time.perf_counter()

    # Ex√©cuter la fonction
    result = func(*args, **kwargs)

    # Mesurer les performances
    end_time = time.perf_counter()
    execution_time_ms = (end_time - start_time) * 1000

    # M√©moire
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    memory_usage_mb = peak / 1024 / 1024

    # CPU (approximatif)
    cpu_after = process.cpu_percent()
    cpu_usage = max(cpu_after - cpu_before, 0)

    metrics = {
        "execution_time_ms": execution_time_ms,
        "memory_usage_mb": memory_usage_mb,
        "cpu_usage_percent": cpu_usage
    }

    return result, metrics

def validate_integration_results(results: Dict[str, Any],
                               expected_metrics: Dict[str, Any]) -> Dict[str, bool]:
    """
    Valide les r√©sultats d'un test d'int√©gration.

    Args:
        results: R√©sultats du test d'int√©gration
        expected_metrics: M√©triques attendues

    Returns:
        Dictionnaire de validation (True/False par m√©trique)
    """
    validation = {}

    # Validation des performances
    if 'performance' in results:
        perf = results['performance']
        for metric, threshold in PERFORMANCE_BENCHMARKS.items():
            if metric.replace('_max_', '_') in perf:
                actual = perf[metric.replace('_max_', '_')]
                validation[f"performance_{metric}"] = actual <= threshold

    # Validation des seuils d'int√©gration
    for threshold_name, threshold_value in INTEGRATION_VALIDATION_THRESHOLDS.items():
        if threshold_name in results:
            actual = results[threshold_name]
            if "max" in threshold_name:
                validation[threshold_name] = actual <= threshold_value
            elif "min" in threshold_name:
                validation[threshold_name] = actual >= threshold_value
            else:
                # Pour les valeurs exactes (comme compliance)
                validation[threshold_name] = abs(actual - threshold_value) < 0.01

    return validation

def log_integration_test_summary(test_name: str, results: Dict[str, Any],
                                validation: Dict[str, bool]) -> None:
    """
    Log un r√©sum√© d√©taill√© des r√©sultats d'int√©gration.

    Args:
        test_name: Nom du test d'int√©gration
        results: R√©sultats d√©taill√©s
        validation: R√©sultats de validation
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"R√âSUM√â TEST D'INT√âGRATION: {test_name}")
    logger.info(f"{'='*60}")

    # Statut g√©n√©ral
    all_passed = all(validation.values()) if validation else False
    status_icon = "üéâ" if all_passed else "‚ö†Ô∏è"
    logger.info(f"{status_icon} STATUT G√âN√âRAL: {'SUCC√àS' if all_passed else 'PARTIELLEMENT R√âUSSI'}")

    # D√©tails des validations
    if validation:
        logger.info(f"\nüìä VALIDATIONS ({sum(validation.values())}/{len(validation)} r√©ussies):")
        for metric, passed in validation.items():
            icon = "‚úÖ" if passed else "‚ùå"
            logger.info(f"  {icon} {metric}")

    # M√©triques de performance si disponibles
    if 'performance' in results:
        perf = results['performance']
        logger.info(f"\n‚ö° M√âTRIQUES DE PERFORMANCE:")
        for metric, value in perf.items():
            logger.info(f"  ‚Ä¢ {metric}: {value}")

    # R√©sultats cl√©s
    if 'summary' in results:
        summary = results['summary']
        logger.info(f"\nüîç R√âSULTATS CL√âS:")
        for key, value in summary.items():
            logger.info(f"  ‚Ä¢ {key}: {value}")

    logger.info(f"{'='*60}\n")

# Classes d'exception sp√©cialis√©es pour les tests d'int√©gration
class IntegrationTestError(Exception):
    """Exception de base pour les tests d'int√©gration."""
    pass

class PerformanceBenchmarkError(IntegrationTestError):
    """Exception lev√©e quand les benchmarks de performance ne sont pas atteints."""
    pass

class ValidationThresholdError(IntegrationTestError):
    """Exception lev√©e quand les seuils de validation ne sont pas respect√©s."""
    pass

# Messages standardis√©s pour les tests d'int√©gration
INTEGRATION_TEST_MESSAGES = {
    'WORKFLOW_START': "üöÄ D√©marrage test workflow complet",
    'COMPONENT_INTEGRATION': "üîó Test int√©gration composants",
    'PERFORMANCE_CHECK': "‚ö° V√©rification performance",
    'DATA_FLOW_VALIDATION': "üìä Validation flux de donn√©es",
    'CONFIGURATION_TEST': "‚öôÔ∏è Test configuration syst√®me",
    'STRESS_TEST': "üí™ Test de stress",
    'SCENARIO_SIMULATION': "üé≠ Simulation sc√©nario",
    'INTEGRATION_SUCCESS': "üéâ Int√©gration r√©ussie",
    'INTEGRATION_WARNING': "‚ö†Ô∏è Int√©gration partiellement r√©ussie",
    'INTEGRATION_FAILED': "‚ùå √âchec int√©gration"
}
