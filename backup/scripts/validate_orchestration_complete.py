#!/usr/bin/env python3
"""
Script de validation finale pour la tÃ¢che 8.3.1 - Valider orchestration complÃ¨te.
VÃ©rifie que toutes les sous-tÃ¢ches sont implÃ©mentÃ©es et fonctionnelles.
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime

# Add src to path
sys.path.insert(0, '/home/morningstar/Documents/trading/ADAN/src')

def check_file_exists(filepath, description):
    """VÃ©rifie qu'un fichier existe"""
    if os.path.exists(filepath):
        print(f"âœ… {description}: {filepath}")
        return True
    else:
        print(f"âŒ {description}: {filepath} - MANQUANT")
        return False

def check_test_passes(test_script, description):
    """VÃ©rifie qu'un test passe"""
    try:
        result = os.system(f"python {test_script} > /dev/null 2>&1")
        if result == 0:
            print(f"âœ… {description}: RÃ‰USSI")
            return True
        else:
            print(f"âŒ {description}: Ã‰CHEC")
            return False
    except Exception as e:
        print(f"âŒ {description}: ERREUR - {e}")
        return False

def check_benchmark_results():
    """VÃ©rifie que les rÃ©sultats de benchmark existent"""
    logs_dir = Path("logs")

    # Cherche les fichiers de benchmark rÃ©cents
    orchestration_files = list(logs_dir.glob("orchestration_benchmark_*.json"))
    parallel_files = list(logs_dir.glob("parallel_orchestration_benchmark_*.json"))

    has_orchestration = len(orchestration_files) > 0
    has_parallel = len(parallel_files) > 0

    if has_orchestration:
        latest_orch = max(orchestration_files, key=lambda f: f.stat().st_mtime)
        print(f"âœ… Benchmark orchestration: {latest_orch.name}")
    else:
        print("âŒ Benchmark orchestration: MANQUANT")

    if has_parallel:
        latest_parallel = max(parallel_files, key=lambda f: f.stat().st_mtime)
        print(f"âœ… Benchmark parallÃ¨le: {latest_parallel.name}")
    else:
        print("âŒ Benchmark parallÃ¨le: MANQUANT")

    return has_orchestration and has_parallel

def validate_orchestration_implementation():
    """Valide l'implÃ©mentation complÃ¨te de l'orchestration"""
    print("ğŸš€ Validation Orchestration Multi-Environnements")
    print("=" * 60)

    validation_results = {
        'timestamp': datetime.now().isoformat(),
        'task': '8.3.1 - Valider orchestration complÃ¨te',
        'subtasks': {},
        'overall_status': 'PENDING'
    }

    # 1. VÃ©rifier les fichiers d'implÃ©mentation
    print("\nğŸ“ 1. FICHIERS D'IMPLÃ‰MENTATION")
    print("-" * 40)

    impl_files = [
        ("ADAN/src/adan_trading_bot/training/training_orchestrator.py", "TrainingOrchestrator"),
        ("ADAN/src/adan_trading_bot/online_learning_agent.py", "OnlineLearningAgent avec buffer"),
        ("ADAN/src/adan_trading_bot/environment/dynamic_behavior_engine.py", "DBE avec persistance")
    ]

    impl_status = all(check_file_exists(f, desc) for f, desc in impl_files)
    validation_results['subtasks']['implementation_files'] = impl_status

    # 2. VÃ©rifier les tests unitaires
    print("\nğŸ§ª 2. TESTS UNITAIRES")
    print("-" * 40)

    test_files = [
        ("scripts/test_full_orchestration.py", "Test orchestration complÃ¨te"),
        ("scripts/test_dbe_state_persistence.py", "Test persistance DBE"),
        ("scripts/test_continuous_experience_buffer.py", "Test buffer continu")
    ]

    test_status = all(check_test_passes(f, desc) for f, desc in test_files)
    validation_results['subtasks']['unit_tests'] = test_status

    # 3. VÃ©rifier la stabilitÃ© 4 environnements
    print("\nğŸ”„ 3. STABILITÃ‰ 4 ENVIRONNEMENTS")
    print("-" * 40)

    stability_status = check_test_passes(
        "scripts/test_full_orchestration.py",
        "StabilitÃ© 4 environnements simultanÃ©s"
    )
    validation_results['subtasks']['stability_4_envs'] = stability_status

    # 4. VÃ©rifier les mesures de performance
    print("\nğŸ“Š 4. MESURES DE PERFORMANCE")
    print("-" * 40)

    benchmark_status = check_benchmark_results()
    validation_results['subtasks']['performance_benchmarks'] = benchmark_status

    # 5. VÃ©rifier l'utilisation des ressources
    print("\nğŸ’¾ 5. UTILISATION DES RESSOURCES")
    print("-" * 40)

    resource_files = [
        ("logs/orchestration_benchmark_*.json", "MÃ©triques ressources orchestration"),
        ("logs/parallel_orchestration_benchmark_*.json", "MÃ©triques ressources parallÃ¨les")
    ]

    resource_status = True
    for pattern, desc in resource_files:
        files = list(Path(".").glob(pattern))
        if files:
            print(f"âœ… {desc}: {len(files)} fichier(s)")
        else:
            print(f"âŒ {desc}: MANQUANT")
            resource_status = False

    validation_results['subtasks']['resource_validation'] = resource_status

    # 6. VÃ©rifier la documentation des gains
    print("\nğŸ“‹ 6. DOCUMENTATION DES GAINS")
    print("-" * 40)

    doc_status = check_file_exists(
        "ADAN/docs/orchestration_performance_report.md",
        "Rapport de performance"
    )
    validation_results['subtasks']['performance_documentation'] = doc_status

    # 7. RÃ©sumÃ© final
    print("\n" + "=" * 60)
    print("ğŸ“‹ RÃ‰SUMÃ‰ DE VALIDATION")
    print("=" * 60)

    all_subtasks = [
        ("Fichiers d'implÃ©mentation", impl_status),
        ("Tests unitaires", test_status),
        ("StabilitÃ© 4 environnements", stability_status),
        ("Benchmarks de performance", benchmark_status),
        ("Validation des ressources", resource_status),
        ("Documentation des gains", doc_status)
    ]

    passed_count = sum(1 for _, status in all_subtasks if status)
    total_count = len(all_subtasks)

    for desc, status in all_subtasks:
        status_icon = "âœ…" if status else "âŒ"
        print(f"  {status_icon} {desc}")

    overall_success = passed_count == total_count
    validation_results['subtasks']['summary'] = {
        'passed': passed_count,
        'total': total_count,
        'success_rate': passed_count / total_count
    }
    validation_results['overall_status'] = 'SUCCESS' if overall_success else 'PARTIAL'

    print(f"\nğŸ¯ Score: {passed_count}/{total_count} sous-tÃ¢ches validÃ©es")

    if overall_success:
        print("ğŸ‰ TÃ‚CHE 8.3.1 COMPLÃˆTEMENT VALIDÃ‰E !")
        print("âœ… PrÃªt pour le Sprint 9 - Optimisation Performance")
    else:
        print("âš ï¸  VALIDATION PARTIELLE - Quelques Ã©lÃ©ments Ã  complÃ©ter")

    # Sauvegarde du rapport de validation
    validation_file = f"logs/orchestration_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    os.makedirs("logs", exist_ok=True)

    with open(validation_file, 'w') as f:
        json.dump(validation_results, f, indent=2)

    print(f"\nğŸ“ Rapport de validation sauvegardÃ©: {validation_file}")

    return overall_success, validation_results

def main():
    """Fonction principale"""
    success, results = validate_orchestration_implementation()

    if success:
        print("\nğŸš€ VALIDATION RÃ‰USSIE - Sprint 8.3.1 COMPLET")
        return 0
    else:
        print("\nâš ï¸  VALIDATION PARTIELLE - VÃ©rifier les Ã©lÃ©ments manquants")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
