#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Script d'√©valuation final optimis√© pour ADAN.
√âvalue les performances d'un mod√®le entra√Æn√© avec m√©triques d√©taill√©es.
"""
import os
import sys
import argparse
import numpy as np
import pandas as pd
from datetime import datetime

# Ajouter le r√©pertoire parent au path pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.adan_trading_bot.common.utils import get_path, load_config
from src.adan_trading_bot.common.custom_logger import setup_logging
from src.adan_trading_bot.data_processing.feature_engineer import prepare_data_pipeline
from src.adan_trading_bot.environment.multi_asset_env import MultiAssetEnv
from stable_baselines3 import PPO

def calculate_performance_metrics(history, initial_capital):
    """
    Calcule les m√©triques de performance d√©taill√©es.

    Args:
        history: Historique des trades
        initial_capital: Capital initial

    Returns:
        dict: M√©triques de performance
    """
    if not history or len(history) == 0:
        return {}

    # Convertir en DataFrame pour faciliter les calculs
    df = pd.DataFrame(history)

    # Calculs de base
    final_capital = df['capital'].iloc[-1] if 'capital' in df.columns else initial_capital
    total_return = (final_capital - initial_capital) / initial_capital * 100

    # Calculs avanc√©s
    portfolio_values = df['portfolio_value'].values if 'portfolio_value' in df.columns else [initial_capital] * len(df)
    returns = np.diff(portfolio_values) / portfolio_values[:-1]

    # Sharpe Ratio (annualis√©, en supposant 525600 minutes par an pour 1m timeframe)
    if len(returns) > 1 and np.std(returns) > 0:
        sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(525600)
    else:
        sharpe_ratio = 0

    # Maximum Drawdown
    peak = np.maximum.accumulate(portfolio_values)
    drawdown = (portfolio_values - peak) / peak * 100
    max_drawdown = np.min(drawdown)

    # Win Rate (pour les trades avec reward > 0)
    rewards = df['reward'].values if 'reward' in df.columns else []
    positive_rewards = [r for r in rewards if r > 0]
    win_rate = len(positive_rewards) / len(rewards) * 100 if len(rewards) > 0 else 0

    # Dur√©e moyenne des √©pisodes
    episodes = df.groupby(df.index // 1000)  # Approximation
    avg_episode_length = len(df) / len(episodes) if len(episodes) > 0 else len(df)

    return {
        'total_return_pct': total_return,
        'final_capital': final_capital,
        'sharpe_ratio': sharpe_ratio,
        'max_drawdown_pct': max_drawdown,
        'win_rate_pct': win_rate,
        'avg_episode_length': avg_episode_length,
        'total_steps': len(df),
        'avg_reward': np.mean(rewards) if len(rewards) > 0 else 0,
        'cumulative_reward': df['cumulative_reward'].iloc[-1] if 'cumulative_reward' in df.columns else 0
    }

def classify_performance(metrics):
    """
    Classifie la performance du mod√®le.

    Args:
        metrics: M√©triques de performance

    Returns:
        str: Classification (Excellent, Bon, Acceptable, Probl√©matique)
    """
    total_return = metrics.get('total_return_pct', 0)
    sharpe_ratio = metrics.get('sharpe_ratio', 0)
    max_drawdown = abs(metrics.get('max_drawdown_pct', 100))
    win_rate = metrics.get('win_rate_pct', 0)

    # Crit√®res de classification
    if total_return > 15 and sharpe_ratio > 1.5 and max_drawdown < 10 and win_rate > 60:
        return "üèÜ EXCELLENT"
    elif total_return > 8 and sharpe_ratio > 1.0 and max_drawdown < 20 and win_rate > 50:
        return "ü•à BON"
    elif total_return > 2 and sharpe_ratio > 0.5 and max_drawdown < 35 and win_rate > 40:
        return "ü•â ACCEPTABLE"
    else:
        return "‚ö†Ô∏è PROBL√âMATIQUE"

def main():
    """
    Fonction principale d'√©valuation.
    """
    parser = argparse.ArgumentParser(description='ADAN - √âvaluation Final des Performances')

    # Param√®tres principaux
    parser.add_argument('--model_path', type=str, required=True,
                        help='Chemin vers le mod√®le √† √©valuer (.zip)')
    parser.add_argument('--profile', type=str, default='cpu',
                        choices=['cpu', 'gpu'],
                        help='Profil de configuration √† utiliser')

    # Param√®tres d'√©valuation
    parser.add_argument('--episodes', type=int, default=10,
                        help='Nombre d\'√©pisodes d\'√©valuation (d√©faut: 10)')
    parser.add_argument('--max_steps', type=int, default=1000,
                        help='Maximum steps par √©pisode (d√©faut: 1000)')
    parser.add_argument('--initial_capital', type=float, default=15000,
                        help='Capital initial pour l\'√©valuation (d√©faut: 15000)')

    # Options de sortie
    parser.add_argument('--save_report', action='store_true',
                        help='Sauvegarder le rapport d√©taill√©')
    parser.add_argument('--verbose', action='store_true',
                        help='Mode verbose avec logs d√©taill√©s')

    args = parser.parse_args()

    # Configuration des logs
    log_level = 'DEBUG' if args.verbose else 'INFO'
    logger = setup_logging('config/logging_config.yaml', level=log_level)

    # Affichage de d√©marrage
    print("üìä ADAN - √âvaluation des Performances")
    print("=" * 50)
    print(f"ü§ñ Mod√®le: {args.model_path}")
    print(f"üìã Profil: {args.profile.upper()}")
    print(f"üéØ √âpisodes: {args.episodes}")
    print(f"üí∞ Capital initial: ${args.initial_capital:,.2f}")
    print("=" * 50)

    # V√©rifier l'existence du mod√®le
    if not os.path.exists(args.model_path):
        logger.error(f"‚ùå Mod√®le non trouv√©: {args.model_path}")
        sys.exit(1)

    # Charger les configurations
    config_paths = {
        'main': 'config/main_config.yaml',
        'data': f'config/data_config_{args.profile}.yaml',
        'environment': 'config/environment_config.yaml',
        'agent': f'config/agent_config_{args.profile}.yaml'
    }

    try:
        # Charger le mod√®le
        logger.info(f"üîÑ Chargement du mod√®le: {args.model_path}")
        model = PPO.load(args.model_path)

        # Pr√©parer l'environnement de test
        logger.info("üîÑ Pr√©paration de l'environnement de test...")

        # Charger les donn√©es de test
        config = {}
        for key, path in config_paths.items():
            config[key] = load_config(path)

        # Pr√©parer les donn√©es
        df_test, scaler, encoder = prepare_data_pipeline(
            config['data'],
            split='test',
            scaler=None,
            encoder=None
        )

        logger.info(f"üìà Donn√©es de test charg√©es: {df_test.shape}")

        # Cr√©er l'environnement
        env = MultiAssetEnv(
            df_received=df_test,
            config=config,
            scaler=scaler,
            encoder=encoder,
            max_episode_steps_override=args.max_steps
        )

        # Override du capital initial
        env.initial_capital = args.initial_capital

        # √âvaluation
        logger.info(f"üéØ D√©but de l'√©valuation ({args.episodes} √©pisodes)...")

        all_rewards = []
        all_history = []
        episode_results = []

        for episode in range(args.episodes):
            obs, _ = env.reset()
            episode_reward = 0
            episode_steps = 0
            done = False

            while not done and episode_steps < args.max_steps:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, done, truncated, info = env.step(action)
                episode_reward += reward
                episode_steps += 1

                if done or truncated:
                    break

            all_rewards.append(episode_reward)
            all_history.extend(env.history)

            final_portfolio = info.get('portfolio_value', args.initial_capital)
            return_pct = (final_portfolio - args.initial_capital) / args.initial_capital * 100

            episode_results.append({
                'episode': episode + 1,
                'reward': episode_reward,
                'steps': episode_steps,
                'final_portfolio': final_portfolio,
                'return_pct': return_pct
            })

            print(f"üìà √âpisode {episode+1}/{args.episodes}: "
                  f"Reward={episode_reward:.4f}, "
                  f"Steps={episode_steps}, "
                  f"Return={return_pct:.2f}%")

        # Calcul des m√©triques globales
        metrics = calculate_performance_metrics(all_history, args.initial_capital)
        classification = classify_performance(metrics)

        # Affichage des r√©sultats
        print("\n" + "=" * 60)
        print("üìä R√âSULTATS DE L'√âVALUATION")
        print("=" * 60)

        print(f"\nüéØ PERFORMANCE GLOBALE: {classification}")
        print("-" * 40)
        print(f"üí∞ Rendement Total: {metrics.get('total_return_pct', 0):.2f}%")
        print(f"üíé Capital Final: ${metrics.get('final_capital', args.initial_capital):,.2f}")
        print(f"üìà Ratio de Sharpe: {metrics.get('sharpe_ratio', 0):.3f}")
        print(f"üìâ Drawdown Max: {metrics.get('max_drawdown_pct', 0):.2f}%")
        print(f"üéØ Taux de Victoire: {metrics.get('win_rate_pct', 0):.1f}%")
        print(f"üîÑ R√©compense Moyenne: {metrics.get('avg_reward', 0):.4f}")

        print(f"\nüìä STATISTIQUES D'√âPISODES:")
        print("-" * 40)
        rewards_array = np.array(all_rewards)
        print(f"üèÜ Meilleure R√©compense: {np.max(rewards_array):.4f}")
        print(f"üìâ Pire R√©compense: {np.min(rewards_array):.4f}")
        print(f"üìä R√©compense Moyenne: {np.mean(rewards_array):.4f}")
        print(f"üìè √âcart-Type: {np.std(rewards_array):.4f}")
        print(f"‚è±Ô∏è  Dur√©e Moyenne d'√âpisode: {metrics.get('avg_episode_length', 0):.0f} steps")

        # Sauvegarde du rapport si demand√©
        if args.save_report:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_name = os.path.basename(args.model_path).replace('.zip', '')
            report_path = f"reports/evaluation_{model_name}_{timestamp}.json"

            os.makedirs("reports", exist_ok=True)

            report_data = {
                'model_path': args.model_path,
                'evaluation_date': timestamp,
                'parameters': vars(args),
                'metrics': metrics,
                'classification': classification,
                'episode_results': episode_results,
                'episode_stats': {
                    'mean_reward': float(np.mean(rewards_array)),
                    'std_reward': float(np.std(rewards_array)),
                    'min_reward': float(np.min(rewards_array)),
                    'max_reward': float(np.max(rewards_array))
                }
            }

            import json
            with open(report_path, 'w') as f:
                json.dump(report_data, f, indent=2)

            print(f"\nüíæ Rapport sauvegard√©: {report_path}")

        print("\n" + "=" * 60)

        # Recommandations
        print("üí° RECOMMANDATIONS:")
        if "EXCELLENT" in classification:
            print("‚úÖ Mod√®le pr√™t pour le trading en production!")
            print("üöÄ Consid√©rez un entra√Ænement plus long pour optimiser davantage.")
        elif "BON" in classification:
            print("‚úÖ Mod√®le acceptable pour le trading prudent.")
            print("üîß Ajustez les param√®tres d'entra√Ænement pour am√©liorer.")
        elif "ACCEPTABLE" in classification:
            print("‚ö†Ô∏è Mod√®le n√©cessite des am√©liorations avant production.")
            print("üîß Augmentez le nombre de timesteps d'entra√Ænement.")
        else:
            print("‚ùå Mod√®le non recommand√© pour le trading.")
            print("üîÑ Re-entra√Ænement n√©cessaire avec param√®tres optimis√©s.")

    except Exception as e:
        logger.error(f"‚ùå Erreur pendant l'√©valuation: {str(e)}")
        print(f"\nüí• ERREUR: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
